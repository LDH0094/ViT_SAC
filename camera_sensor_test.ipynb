{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dinov2.eval.depth.models import build_depther\n",
    "import urllib\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "class CenterPadding(torch.nn.Module):\n",
    "    def __init__(self, multiple):\n",
    "        super().__init__()\n",
    "        self.multiple = multiple\n",
    "\n",
    "    def _get_pad(self, size):\n",
    "        new_size = math.ceil(size / self.multiple) * self.multiple\n",
    "        pad_size = new_size - size\n",
    "        pad_size_left = pad_size // 2\n",
    "        pad_size_right = pad_size - pad_size_left\n",
    "        return pad_size_left, pad_size_right\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, x):\n",
    "        pads = list(itertools.chain.from_iterable(self._get_pad(m) for m in x.shape[:1:-1]))\n",
    "        output = F.pad(x, pads)\n",
    "        return output\n",
    "\n",
    "\n",
    "def create_depther(cfg, backbone_model, backbone_size, head_type):\n",
    "    train_cfg = cfg.get(\"train_cfg\")\n",
    "    test_cfg = cfg.get(\"test_cfg\")\n",
    "    depther = build_depther(cfg.model, train_cfg=train_cfg, test_cfg=test_cfg)\n",
    "\n",
    "    depther.backbone.forward = partial(\n",
    "        backbone_model.get_intermediate_layers,\n",
    "        n=cfg.model.backbone.out_indices,\n",
    "        reshape=True,\n",
    "        return_class_token=cfg.model.backbone.output_cls_token,\n",
    "        norm=cfg.model.backbone.final_norm,\n",
    "    )\n",
    "\n",
    "    if hasattr(backbone_model, \"patch_size\"):\n",
    "        depther.backbone.register_forward_pre_hook(lambda _, x: CenterPadding(backbone_model.patch_size)(x[0]))\n",
    "\n",
    "    return depther"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/ideoghyeon/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "data": {
      "text/plain": "DinoVisionTransformer(\n  (patch_embed): PatchEmbed(\n    (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n    (norm): Identity()\n  )\n  (blocks): ModuleList(\n    (0-11): 12 x NestedTensorBlock(\n      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (attn): MemEffAttention(\n        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): LayerScale()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n        (act): GELU(approximate='none')\n        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n        (drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): LayerScale()\n      (drop_path2): Identity()\n    )\n  )\n  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n  (head): Identity()\n)"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BACKBONE_SIZE = \"small\" # in (\"small\", \"base\", \"large\" or \"giant\")\n",
    "\n",
    "\n",
    "backbone_archs = {\n",
    "    \"small\": \"vits14\",\n",
    "    \"base\": \"vitb14\",\n",
    "    \"large\": \"vitl14\",\n",
    "    \"giant\": \"vitg14\",\n",
    "}\n",
    "backbone_arch = backbone_archs[BACKBONE_SIZE]\n",
    "backbone_name = f\"dinov2_{backbone_arch}\"\n",
    "\n",
    "backbone_model = torch.hub.load(repo_or_dir=\"facebookresearch/dinov2\", model=backbone_name)\n",
    "backbone_model.to(\"cpu\")\n",
    "backbone_model.eval()\n",
    "# backbone_model.cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from http path: https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_kitti_linear4_head.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": "DepthEncoderDecoder(\n  (backbone): DinoVisionTransformer()\n  (decode_head): BNHead(\n    align_corners=False\n    (loss_decode): ModuleList(\n      (0): SigLoss()\n      (1): GradientLoss()\n    )\n    (softmax): Softmax(dim=1)\n    (conv_depth): Conv2d(3072, 256, kernel_size=(1, 1), stride=(1, 1))\n    (relu): ReLU()\n    (sigmoid): Sigmoid()\n  )\n)"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "\n",
    "import mmcv\n",
    "from mmcv.runner import load_checkpoint\n",
    "\n",
    "\n",
    "def load_config_from_url(url: str) -> str:\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        return f.read().decode()\n",
    "\n",
    "\n",
    "HEAD_DATASET = \"kitti\" # in (\"nyu\", \"kitti\")\n",
    "HEAD_TYPE = \"linear4\" # in (\"linear\", \"linear4\", \"dpt\")\n",
    "\n",
    "# kitti + dpt = very slow\n",
    "# kitti + linear = fast\n",
    "# kitti + linear4 = similar to linear\n",
    "\n",
    "\n",
    "DINOV2_BASE_URL = \"https://dl.fbaipublicfiles.com/dinov2\"\n",
    "head_config_url = f\"{DINOV2_BASE_URL}/{backbone_name}/{backbone_name}_{HEAD_DATASET}_{HEAD_TYPE}_config.py\"\n",
    "head_checkpoint_url = f\"{DINOV2_BASE_URL}/{backbone_name}/{backbone_name}_{HEAD_DATASET}_{HEAD_TYPE}_head.pth\"\n",
    "\n",
    "cfg_str = load_config_from_url(head_config_url)\n",
    "cfg = mmcv.Config.fromstring(cfg_str, file_format=\".py\")\n",
    "\n",
    "model = create_depther(\n",
    "    cfg,\n",
    "    backbone_model=backbone_model,\n",
    "    backbone_size=BACKBONE_SIZE,\n",
    "    head_type=HEAD_TYPE,\n",
    ")\n",
    "\n",
    "load_checkpoint(model, head_checkpoint_url, map_location=\"cpu\")\n",
    "model.eval()\n",
    "# model.cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building Pipeline for transform"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def make_depth_transform() -> transforms.Compose:\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        lambda x: 255.0 * x[:3], # Discard alpha component and scale by 255\n",
    "        transforms.Normalize(\n",
    "            mean=(123.675, 116.28, 103.53),\n",
    "            std=(58.395, 57.12, 57.375),\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "\n",
    "def render_depth(values, colormap_name=\"magma_r\") -> Image:\n",
    "    min_value, max_value = values.min(), values.max()\n",
    "    normalized_values = (values - min_value) / (max_value - min_value)\n",
    "\n",
    "    colormap = matplotlib.colormaps[colormap_name]\n",
    "    colors = colormap(normalized_values, bytes=True) # ((1)xhxwx4)\n",
    "    colors = colors[:, :, :3] # Discard alpha component\n",
    "    return Image.fromarray(colors)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Drone Enviornment Setup\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "from collections import deque\n",
    "from replaybuffer import ReplayBuffers\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name= None, base_port=5004)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "env.reset()\n",
    "behavior_names = list(env.behavior_specs.keys())\n",
    "behavior_name = behavior_names[0]\n",
    "decision_steps, terminal_steps = env.get_steps(behavior_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get the length of the space size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "# initial states\n",
    "N_STATES = len(decision_steps.obs[0][0]) + len(decision_steps.obs[1][0])\n",
    "IMAGE_HEIGHT = 84 # no use in code just here to check the sizes\n",
    "IMAGE_WIDTH = 84"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-30.26,   3.42,  -0.67,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,\n         0.  ,   0.  ,   0.  ,   0.  , -30.26,   3.42,  -0.67],\n      dtype=float32)"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_steps.obs[1][0] # Agent's position (x,y,z) + Target's position (x,y,z)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[0.27450982, 0.2627451 , 0.30588236],\n        [0.49411765, 0.6       , 0.74509805],\n        [0.49411765, 0.6       , 0.74509805],\n        ...,\n        [0.5058824 , 0.6156863 , 0.7647059 ],\n        [0.50980395, 0.61960787, 0.7647059 ],\n        [0.50980395, 0.61960787, 0.7647059 ]],\n\n       [[0.27450982, 0.2627451 , 0.30588236],\n        [0.49803922, 0.6039216 , 0.7490196 ],\n        [0.49803922, 0.6039216 , 0.7490196 ],\n        ...,\n        [0.50980395, 0.62352943, 0.76862746],\n        [0.5137255 , 0.62352943, 0.76862746],\n        [0.5137255 , 0.62352943, 0.77254903]],\n\n       [[0.27450982, 0.2627451 , 0.30588236],\n        [0.5019608 , 0.60784316, 0.7529412 ],\n        [0.5019608 , 0.60784316, 0.7529412 ],\n        ...,\n        [0.5137255 , 0.627451  , 0.77254903],\n        [0.5137255 , 0.627451  , 0.7764706 ],\n        [0.5176471 , 0.627451  , 0.7764706 ]],\n\n       ...,\n\n       [[0.4392157 , 0.60784316, 0.24705882],\n        [0.4509804 , 0.62352943, 0.2509804 ],\n        [0.44705883, 0.6156863 , 0.2509804 ],\n        ...,\n        [0.6313726 , 0.81960785, 0.32941177],\n        [0.67058825, 0.8745098 , 0.3372549 ],\n        [0.6745098 , 0.8745098 , 0.33333334]],\n\n       [[0.40784314, 0.5803922 , 0.24313726],\n        [0.39607844, 0.53333336, 0.2509804 ],\n        [0.39607844, 0.5294118 , 0.24705882],\n        ...,\n        [0.5764706 , 0.74509805, 0.32156864],\n        [0.6156863 , 0.7882353 , 0.34117648],\n        [0.6117647 , 0.78431374, 0.33333334]],\n\n       [[0.40392157, 0.57254905, 0.23137255],\n        [0.37254903, 0.5019608 , 0.23529412],\n        [0.36862746, 0.5019608 , 0.23529412],\n        ...,\n        [0.5568628 , 0.72156864, 0.3137255 ],\n        [0.5803922 , 0.74509805, 0.32156864],\n        [0.5764706 , 0.7372549 , 0.31764707]]], dtype=float32)"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_camera_image = decision_steps.obs[0][0] # Agent's Camera view\n",
    "np_camera_image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=84x84>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[46.4520, 46.4520, 46.4520,  ..., 40.4250, 40.4250, 40.4250],\n",
      "        [46.4520, 46.4520, 46.4520,  ..., 40.4250, 40.4250, 40.4250],\n",
      "        [46.4520, 46.4520, 46.4520,  ..., 40.4250, 40.4250, 40.4250],\n",
      "        ...,\n",
      "        [19.9871, 19.9871, 19.9871,  ..., 20.1432, 20.1432, 20.1432],\n",
      "        [19.9871, 19.9871, 19.9871,  ..., 20.1432, 20.1432, 20.1432],\n",
      "        [19.9871, 19.9871, 19.9871,  ..., 20.1432, 20.1432, 20.1432]])\n"
     ]
    }
   ],
   "source": [
    "image_tensor = np_camera_image\n",
    "transform = make_depth_transform() # Apply the transformation pipeline\n",
    "\n",
    "transformed_image = transform(image_tensor)\n",
    "# batch = transformed_image.unsqueeze(0).cuda() # Make a batch of one image\n",
    "batch = transformed_image.unsqueeze(0) # Make a batch of one image\n",
    "\n",
    "with torch.inference_mode():\n",
    "    result = model.whole_inference(batch, img_meta=None, rescale=True)\n",
    "\n",
    "# Question why is result 3d??\n",
    "treated_image_2d = result.squeeze().cpu()\n",
    "depth_image = render_depth(result.squeeze().cpu())\n",
    "display(depth_image)\n",
    "print(result.squeeze().cpu())\n",
    "# treated_image = treated_image_2d.flatten()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "action_state = decision_steps.obs[1][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Image  7071\n",
      "[ 46.452003  46.452003  46.452003 ... -30.26       3.42      -0.67    ]\n"
     ]
    }
   ],
   "source": [
    "flattened_image = treated_image_2d.flatten()\n",
    "\n",
    "flattened_image\n",
    "state = np.concatenate((flattened_image,action_state ), 0)\n",
    "print('Length of Image ', len(state))\n",
    "print(state)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = decision_steps.obs[0][0]  # Agent's Camera view\n",
    "y = decision_steps.obs[1][0]  # Agent's position (x,y,z) + Target's position (x,y,z)\n",
    "state = np.concatenate((x, y), 0)\n",
    "state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env.reset()\n",
    "while True:\n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    x = decision_steps.obs[0][0] # Ray Perception 3D\n",
    "    y = decision_steps.obs[1][0] # Agent's camera\n",
    "    print(y)\n",
    "    action = ActionTuple(np.array([0, 0, 1], dtype = np.float32)) # for testing 0,0,1 simultaneuosly\n",
    "    env.set_actions(behavior_name, action)\n",
    "    # move the agent along with the action.\n",
    "    env.step()\n",
    "    action = action._continuous # converting ActionTuple to array\n",
    "    next_decision_steps, next_terminal_steps = env.get_steps(behavior_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "env.close()\n",
    "# TODO: OpenCV!!!\n",
    "# Combine the two SAC + DINO2\n",
    "# 256 x 256 looking good"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_states()\n",
    "\n",
    "agent_target_states = [...]\n",
    "np_camera_image = [[...]]\n",
    "\n",
    "treated_image = dinov2(np_camera_image) # 384\n",
    "\n",
    "states_for_training = np.concat(agent_target_states, treated_image)\n",
    "\n",
    "SAC same\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
