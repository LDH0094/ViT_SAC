{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dinov2.eval.depth.models import build_depther\n",
    "import urllib\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "class CenterPadding(torch.nn.Module):\n",
    "    def __init__(self, multiple):\n",
    "        super().__init__()\n",
    "        self.multiple = multiple\n",
    "\n",
    "    def _get_pad(self, size):\n",
    "        new_size = math.ceil(size / self.multiple) * self.multiple\n",
    "        pad_size = new_size - size\n",
    "        pad_size_left = pad_size // 2\n",
    "        pad_size_right = pad_size - pad_size_left\n",
    "        return pad_size_left, pad_size_right\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, x):\n",
    "        pads = list(itertools.chain.from_iterable(self._get_pad(m) for m in x.shape[:1:-1]))\n",
    "        output = F.pad(x, pads)\n",
    "        return output\n",
    "\n",
    "\n",
    "def create_depther(cfg, backbone_model, backbone_size, head_type):\n",
    "    train_cfg = cfg.get(\"train_cfg\")\n",
    "    test_cfg = cfg.get(\"test_cfg\")\n",
    "    depther = build_depther(cfg.model, train_cfg=train_cfg, test_cfg=test_cfg)\n",
    "\n",
    "    depther.backbone.forward = partial(\n",
    "        backbone_model.get_intermediate_layers,\n",
    "        n=cfg.model.backbone.out_indices,\n",
    "        reshape=True,\n",
    "        return_class_token=cfg.model.backbone.output_cls_token,\n",
    "        norm=cfg.model.backbone.final_norm,\n",
    "    )\n",
    "\n",
    "    if hasattr(backbone_model, \"patch_size\"):\n",
    "        depther.backbone.register_forward_pre_hook(lambda _, x: CenterPadding(backbone_model.patch_size)(x[0]))\n",
    "\n",
    "    return depther"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/ideoghyeon/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/Users/ideoghyeon/Library/CloudStorage/OneDrive-ateneo.edu/Thesis/Lee_RL/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/Users/ideoghyeon/Library/CloudStorage/OneDrive-ateneo.edu/Thesis/Lee_RL/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/Users/ideoghyeon/Library/CloudStorage/OneDrive-ateneo.edu/Thesis/Lee_RL/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "DinoVisionTransformer(\n  (patch_embed): PatchEmbed(\n    (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n    (norm): Identity()\n  )\n  (blocks): ModuleList(\n    (0-11): 12 x NestedTensorBlock(\n      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (attn): MemEffAttention(\n        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): LayerScale()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n        (act): GELU(approximate='none')\n        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n        (drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): LayerScale()\n      (drop_path2): Identity()\n    )\n  )\n  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n  (head): Identity()\n)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BACKBONE_SIZE = \"small\" # in (\"small\", \"base\", \"large\" or \"giant\")\n",
    "\n",
    "\n",
    "backbone_archs = {\n",
    "    \"small\": \"vits14\",\n",
    "    \"base\": \"vitb14\",\n",
    "    \"large\": \"vitl14\",\n",
    "    \"giant\": \"vitg14\",\n",
    "}\n",
    "backbone_arch = backbone_archs[BACKBONE_SIZE]\n",
    "backbone_name = f\"dinov2_{backbone_arch}\"\n",
    "\n",
    "backbone_model = torch.hub.load(repo_or_dir=\"facebookresearch/dinov2\", model=backbone_name)\n",
    "backbone_model.to(\"cpu\")\n",
    "backbone_model.eval()\n",
    "# backbone_model.cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from http path: https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_nyu_linear4_head.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": "DepthEncoderDecoder(\n  (backbone): DinoVisionTransformer()\n  (decode_head): BNHead(\n    align_corners=False\n    (loss_decode): ModuleList(\n      (0): SigLoss()\n      (1): GradientLoss()\n    )\n    (softmax): Softmax(dim=1)\n    (conv_depth): Conv2d(3072, 256, kernel_size=(1, 1), stride=(1, 1))\n    (relu): ReLU()\n    (sigmoid): Sigmoid()\n  )\n)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "\n",
    "import mmcv\n",
    "from mmcv.runner import load_checkpoint\n",
    "\n",
    "\n",
    "def load_config_from_url(url: str) -> str:\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        return f.read().decode()\n",
    "\n",
    "\n",
    "HEAD_DATASET = \"nyu\" # in (\"nyu\", \"kitti\")\n",
    "HEAD_TYPE = \"linear4\" # in (\"linear\", \"linear4\", \"dpt\")\n",
    "\n",
    "# kitti + dpt = very slow\n",
    "# kitti + linear = fast\n",
    "# kitti + linear4 = similar to linear\n",
    "\n",
    "\n",
    "DINOV2_BASE_URL = \"https://dl.fbaipublicfiles.com/dinov2\"\n",
    "head_config_url = f\"{DINOV2_BASE_URL}/{backbone_name}/{backbone_name}_{HEAD_DATASET}_{HEAD_TYPE}_config.py\"\n",
    "head_checkpoint_url = f\"{DINOV2_BASE_URL}/{backbone_name}/{backbone_name}_{HEAD_DATASET}_{HEAD_TYPE}_head.pth\"\n",
    "\n",
    "cfg_str = load_config_from_url(head_config_url)\n",
    "cfg = mmcv.Config.fromstring(cfg_str, file_format=\".py\")\n",
    "\n",
    "model = create_depther(\n",
    "    cfg,\n",
    "    backbone_model=backbone_model,\n",
    "    backbone_size=BACKBONE_SIZE,\n",
    "    head_type=HEAD_TYPE,\n",
    ")\n",
    "\n",
    "load_checkpoint(model, head_checkpoint_url, map_location=\"cpu\")\n",
    "model.eval()\n",
    "# model.cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building Pipeline for transform"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def make_depth_transform() -> transforms.Compose:\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        lambda x: 255.0 * x[:3], # Discard alpha component and scale by 255\n",
    "        transforms.Normalize(\n",
    "            mean=(123.675, 116.28, 103.53),\n",
    "            std=(58.395, 57.12, 57.375),\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "\n",
    "def render_depth(values, colormap_name=\"magma_r\") -> Image:\n",
    "    min_value, max_value = values.min(), values.max()\n",
    "    normalized_values = (values - min_value) / (max_value - min_value)\n",
    "\n",
    "    colormap = matplotlib.colormaps[colormap_name]\n",
    "    colors = colormap(normalized_values, bytes=True) # ((1)xhxwx4)\n",
    "    colors = colors[:, :, :3] # Discard alpha component\n",
    "    return Image.fromarray(colors)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Drone Enviornment Setup\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "from collections import deque\n",
    "from replaybuffer import ReplayBuffers\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name= None, base_port=5004)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "env.reset()\n",
    "behavior_names = list(env.behavior_specs.keys())\n",
    "behavior_name = behavior_names[0]\n",
    "decision_steps, terminal_steps = env.get_steps(behavior_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get the length of the space size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# initial states\n",
    "N_STATES = len(decision_steps.obs[0][0]) + len(decision_steps.obs[1][0])\n",
    "IMAGE_HEIGHT = 84 # no use in code just here to check the sizes\n",
    "IMAGE_WIDTH = 84"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-30.26,   3.42,  -0.67,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,\n         0.  ,   0.  ,   0.  ,   0.  , -30.26,   3.42,  -0.67],\n      dtype=float32)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_steps.obs[1][0] # Agent's position (x,y,z) + Target's position (x,y,z)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[0.27450982, 0.2627451 , 0.30588236],\n        [0.49411765, 0.6       , 0.74509805],\n        [0.49411765, 0.6       , 0.74509805],\n        ...,\n        [0.5058824 , 0.6156863 , 0.7647059 ],\n        [0.50980395, 0.61960787, 0.7647059 ],\n        [0.50980395, 0.61960787, 0.7647059 ]],\n\n       [[0.27450982, 0.2627451 , 0.30588236],\n        [0.49803922, 0.6039216 , 0.7490196 ],\n        [0.49803922, 0.6039216 , 0.7490196 ],\n        ...,\n        [0.50980395, 0.62352943, 0.76862746],\n        [0.5137255 , 0.62352943, 0.76862746],\n        [0.5137255 , 0.62352943, 0.77254903]],\n\n       [[0.27450982, 0.2627451 , 0.30588236],\n        [0.5019608 , 0.60784316, 0.7529412 ],\n        [0.5019608 , 0.60784316, 0.7529412 ],\n        ...,\n        [0.5137255 , 0.627451  , 0.77254903],\n        [0.5137255 , 0.627451  , 0.7764706 ],\n        [0.5176471 , 0.627451  , 0.7764706 ]],\n\n       ...,\n\n       [[0.4392157 , 0.60784316, 0.24705882],\n        [0.4509804 , 0.62352943, 0.2509804 ],\n        [0.44705883, 0.6156863 , 0.2509804 ],\n        ...,\n        [0.6313726 , 0.81960785, 0.32941177],\n        [0.67058825, 0.8745098 , 0.3372549 ],\n        [0.6745098 , 0.8745098 , 0.33333334]],\n\n       [[0.40784314, 0.5803922 , 0.24313726],\n        [0.39607844, 0.53333336, 0.2509804 ],\n        [0.39607844, 0.5294118 , 0.24705882],\n        ...,\n        [0.5764706 , 0.74509805, 0.32156864],\n        [0.6156863 , 0.7882353 , 0.34117648],\n        [0.6117647 , 0.78431374, 0.33333334]],\n\n       [[0.40392157, 0.57254905, 0.23137255],\n        [0.37254903, 0.5019608 , 0.23529412],\n        [0.36862746, 0.5019608 , 0.23529412],\n        ...,\n        [0.5568628 , 0.72156864, 0.3137255 ],\n        [0.5803922 , 0.74509805, 0.32156864],\n        [0.5764706 , 0.7372549 , 0.31764707]]], dtype=float32)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_camera_image = decision_steps.obs[0][0] # Agent's Camera view\n",
    "np_camera_image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=84x84>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFQAAABUCAIAAACTCYeWAAAXxElEQVR4nO187XIrybFcZlb3gDwr6fo5/JR+A7+oHVchrXYPOdOV/lHdgwHAc7TXEf5jaYKBAEACM9lZn1k95P/87/8DlyONw9wTe+L3gX/s/sfhv+/j12P/K/7xd/31V//v38b/+n3/z8/j70bCaWTobWt/3tqf3+Ivb/yPd/zlW/7pHW/v6JtikwSGGJxnsS9nxPmmn64kYRvD/i3337H/xt9+499+819/z//8OP5mZOZhJ6nQFro13cQW7EIXghQAQgB8P8/90Otb/zrHv8H/qx7/Bv//7EgYl/BWT/5ItHv9kvX8HrfsBGCM/+vLa8/oCRoERKzYDIEA9MdWKjGMNDPthIcdNAwkIGLifw7s67MP71eor+8xspAnhj0qehf+y98PQ4kU62VWwP8h+KfXAkSL8yrJLz7zo8POdIJIjIQHhhGFH8TE/3hc0T5xX3kuYQNG4R9GnqswmXfWm9dkZmQCQpwvvwb/BO+8giK/ngDgWo6fIAdQ15fr50CGswxn4v/qyK9+4VlFOFE/9Z3jtAY753PWOyOZ/AHOr8HHE3gggSQGCz8Ba/0NIVJkfGlOpymaObAPxnAMSGaCaeq/YkhpG0h4OA/k4JHMgT2L6hM58r4W9SaHIS4n/RHtAFrjo/sBwxQdpHjnH8vzz4PU6XLr3CN9DO4D++BxeBA7oYTDGqBM/gH8daKEXVEDOTAOjIE9faT3zD2925k+7viXR8iZSDITkHGt857BP11LEEEkMAjNH5KcMc8PK0DoaV3tYYyB/cBeZ5VlRCIECoRBfIH/+r1n+C7kB3Ln58A+cCRGei+0E7mPBIRWq2/ITOOB/B8dr8wz6GFGkX/6/Fq8tSDP31tGmMjhXdgPfggStFPpFIYggfUlT0YkcFyen8jrp2jf+TGwJ/bhI/M4kRsJZOZBhNjMtAfYMA0+fgb+yecJDzKIwyBA/oEFPL39dD/mwL4XECPZJ3JLE7yuUMc89f08Ald684GjaB/Yh3cjy+YLuZ1EFtvpg1ZyWn4wEiN+fPlNj+AFDOK42zwwl4Ayr6n+yeZXvMn0PrzfrYNIpCxBZOiRefpcBQHj/P5ajorwg2NwIk8fmQ/I7cPUQ8xz/sjy9WgILfhcb6iyPUhM8p8+/2TzdpKXmMdM70mlozAmUpQQ0+YX/wDIKOQDo4z/XA4zK8NVBFk2XzHvSB9A2gkU/nauvnG3/J8gx2vAE6epF+ZaOhEVrrg+/+wKTiz86SOhYYGQu5lGEhfwVF0HIeA4l2MARICT/0QaI5En7Yv5Y2FOLEFh4sc94Zfl66e5pTW9Mk/OUu+LD/woAhh3rxs46Ki/LjZIrVAXtRDnVwkaxFqaQ9YABCUymUYe2A98JJbNY9iHfcBpGACfUv0st+4u+aNrbk+Fm8igRU7+ARIEOcmvgP91CD2ZJ2N4r3fEbg467uAvC1FXJsTAQehciOXz44xzw/uRH7lC/UKegGGCaR92O2ve8rhzCU67ewD/RG/aAAULIDjTGi2QoEAhRJGSmjO/TKeVbwd2TQlJwE6EqMReCyFE4jgrkLo4I7BaN89iNsvbc0W46e0ncsBV+/OedH7E+XPAy8ey2pcMXB0OZ+XDBoVb563hret96JOI9J4+CElN7FITW9W/Fd6xuitj1LlquQeSEDyEqC6Qc72mORR4rH7JHvZ4bONU+ImqwhZT6zHYgz3QA73M6pl5/PQg0Fg5io3q7t1b463pW4vP5J5uIz+IKNhSr3OfyB9XdmA2sCmqXhZR9AwEZp7gC+rAXpHv8cKqTxaQ4AzKnCbZBYmdUKA33HQJt/8c/OnwFfar5m3U5ujYNrx/8reu98E28mOWsWxiP2E/nemBsWLeY8WOuRCJnQj4wVYrjl4jGQDM0HPin7DnDyR2UUV7K9qr0Hgy++HnmG7w2tgG0YQubtKb++FvCSdTioH90Pf0UVzVKdcH40L115LL+c7wWJ9KAHqsI6qNuxBTBVdby5hkE7dSr0vAbrp1fmu4ddy6t0C/1pd38PkI3lMwZ4Wy4ryLmzAahyNzq/zQ2A/sBz8Gdqyg9XDRpT04kyVCKM8ucMF+lmJKq1mt2NMxrZot2cj7B6WJvMV713vXe+e3Dd82vHffbr4JirOzuoIfLwEvL7qSiC4cxhYYZjbhaEjI7O6DR7UcqIavMHOG3AO7kYN7YlTlw1mBJxFlzCdge6BqwSLHuPqOT9NY+K8UBovzrXEL3k7kN7/ffOtoDSIeP7PAv5j9iRyT/EbcNMsWQBwtBrtjygz+VlJMdSPpHDwSHtgPHgNtx4cYla5rQS8mkCfs2Rrc28d8ipozibCfQbGWI3Rr8db13vRt47eG24n8ht4RQQXYXqzpmXkAh2mgqoTy+a6l6BFBdvEmfWSMlX9wF2p9IGtRduw79h0SY8cHABAn/tPPAYyZwx8yGSmz8znIF/N9WgHinFWFbhu/db7f8O1EfkPbFDG1Jz4VNe34IuDh8Ix5optm60MgxC70gU18N0tPqyzkJSztzmHvHq1iLLn78+5rCz9XB5KlwOR+5rYTp52VO3FmxArmavQ9tk9X57fG2w3ftkfkm1SFR+hZR/mC+XyO9i4pT0QzGrGpYHOUBwNj6cnD3lN7ZiTDggEv6XJ2eEOI8dwLj5o6pvcL+JAasvz8YvwQeK5CJ9R4a3zb+G3D+xPyN0UXmxh8KWZfff5J7BORRtAUWyUCYXgmhRLoh0tS5Z4e5ufwZ6oNfpSU6tIznM7kSHQzaYmqsGpkVewjP3zJamJDVusvOpdweumI2EkFe9Ot873hdvN7d79h64jifAt2sYuVtp7B7z8VpAE0Iov/c4HW02EaOBLDOIw9uSeCiEHCJG2UCLn0/Bw4At3IxI4Z54aduRp1AKgswKlJVNW8ok6cVhBsYhd7w1vHW8etY7vhdkPbGJt0k7ZCrpmwX5n/AjAX7QJI9CVpCL5OclzNNHgkPpKfiT0RlaMIDdhKRFZB62xogy0xSK3RzSzdqlfL/KzlxQoKRT4SUjsz3xn2xN54a7x13CrIbYjOuClu0ha8BZsQZCPiKct/meoIiF6ziqqJHUSjSTSCawk82yvuiZbqic+8R7Y0RvCwhmM4G9qOCPSBqgjvbuxTkCjm7/hlPjkiCAUbGYU82Btu3Vt372gb2yZtUW0Wm3BThTrEUuXu4F8Fi0LeVlXf6CCCbkTQ9dMI0jYPw2CQwJzMHKKBYRzCZ84WL6xANMSwSAX6wC724J7cX+q2BEQ2slXLMLsm9gW7EbE4nwbfK6WDZXr1U8LEifwZvPhs91fknS7wXXfYQXflYp4GPljCHxIc4WF2YRe78FkJtlQwS9X5QMGe2ItA6cir5jmrl2fkTbfqIIIzyE/kvnVvDQqqr8TWVpDT5efZ7F+Zv3Le6C40usmdLtj1WM10mr5rnEp4mCMwjF0IIlRRQCX+clk+oVLayX01Rce9gXGKTWpX5Od+m7NL7xeD74hOXWGHKCDO9vRFmGv9pYOY7g30CRhd2eW1FtnophRdyHO2nRxyT8cSwk4VdC7MmnPLCrbKefWLmm2QirydPY/Yz21GwVsBFlvBFtR967iF44ZbR3RGkG2qGfdTpxG8Pz6Av+nZ7MvDVeDloPuF8PMxVFullIbNYe5m0I0mGC82do5iBQV6PZ+SS3SxH/rIFfAqqhXhDW8n5oYebvUN4WhoxfmNrVPFfPBhIuoVmF+Ptj2CX81MMf8F4U1uylAKTjBKKDa71dNL9ljJcsG+IGcimqtSiQN7sA/sTW+XzqfkzT4jOW6B3tyqLS/wAgPRoIA6o1NNjBXYT+gJVEy7FJk/Zr7yXNAiTid/oD0ymC1S9EilGUSan5lBdWUkuWzv6RCUViMSosMYgTbYE2PwSIxru1YuXZiL5LMnjwLPemSQnaq+I9ap57p7Cn0ihl/Ab3pOpJXG2kTuzgwV8mxR+EfIpEOT9kh3uStbqq0Y+zrtmetrxZz8R8Ldt3M+gTXGI6Ihwq0hiuFqyEuprBat1PRYzd2KrPXNxblFHoaA4S+0+y98vphfZr84l7tGyC1GyKGUnElSNo8xDimoSoeC+dI/nuTPxx+U1TXJPRmuBEagUdVQV2fKlRXnmyuY67IBoPZ2oJCTrxuXWtdTw1yKNYJ5t/nl5z1S9bIN0jlEwkYLxci2Vi0I8e5iT5tt2jm3WvkfmM3mLAcu9AZJopFcVfP8y0vqOgGfY9U6SieqY3y126v1Z5/33eeXqXdlKFtkEd7aUFj0IJgeQz2yRbaRQTf5ysMLsSpinyyZc1h0sefK0pqzk1is3ueIr2jWkWvRV+P49dE2PZtD0S6iKa+mXuCjZUQqTILMkSR9DBXspmn2+sH+k+K2QVNd4p3hct1SzYMk0JaOEHN8Ol++wj51R2PORnwXvef7r9nu2ewBBC065El7pIr2yGgZssJsSYIiB5NsR4YymMGsour1+pZ5V5ZSrLTcxadwFavEPAcAa0UAPBdqeREXbBwGiZqH3N+vyd7L1qQXn2cFvCRRTt5itEhFKtxaKsywonS7CTIie2Ss+j9mL1QplyRkEqxcdc3MXar8dOoNZwCLBZhA00OJeumpJ/jhKSvQkLmXID85n8hf3b7d2rPZh7KYP2ObtGC3VIC0GiDzQAqQ40gpm7IrN2WXNvkWfAu8BT8zhKnqNKhRbSGvKWBc8mKFiVM4jssqnBm0QtJVUxnGUAlKOBJdHIld2BMja5seXpv39taPp7dIhybziizYJ+ETNkEhUWI9KyL0GF3RlTf5Jr/Jb8G3xu+Ds1FLqAoSngJLEc5roK6QxmXhs2oUToMqieG84Il5yYo7UY9hdHLXNP7xsh+v9UfmSYumTPrJzimwVYGCSrLCxK9wa6MdscW4RW4jN2mTN03yRWjF/ivhZ0jjChNcrqQL4V3VRz902RfwPu5SInaVoIg9sRNtLUq+1Bbt9rY/vgPRM5jrge07bF2a2MLfsx1skVsb2zHeQrt5mLs9zDQ/k98PbzlnAWcWPKOXjd0QMehBiLSxBcLwMgcBfRadXqXElFjH3ILEYRzmsIe511okV9h7YX57fzX7qTwU1STYLphLcSh2DkDWgTicu3s/RvKtHcM0OErnsUR+H3gLfh9M3zeh50pIxpzXVGHURMIZ0w/WVu0Z8xvv7WZtk8YcNMxBWy33Ka7X3LGGES9m/+1FJCNOQYXtGTAfNutMUZ7N6tkHc4yjH7XT9khWTArqe+D74KZ1QctEx9pJuy/lX8Bh18Z3EWG0VasIJSW5y7dqq+lTVi5lAUtcqsdzUfBlnm/fXqwBE95k++6CBf5p/QxQ3XHYI1uOW8qmXdv2J1fb4CZs4p4lcmNPfCZK7xzGkd7Tu1MgEwINVhXUiKH7JTT5NnPKLMC/AjAlphOzX2gH0PT2Zf9xLyBXIP5yje4LxGaGW8vsR7V6lWurWN6k29BN/D74kbNKNyr9OI09/T3HZ45qY0TiiEY3YdMUi7C0htV0uCur6ViXM8nliiP3tfgKQOPtpRi7cPt1a1bXC9AV682AAtFtZxu8LUGm6qWgO6PLfSioGBWl7mpPTfg+c/yOXZ4BB0AfvAUrVq15oQUEXcxvbVTfQVoEaayEdYfwNYIC/+2fbMt5OE6vEuf+P9XeLbKZw5HAds6bHMqm1pW30fpRMpsFARqBzzz5x575geN3fEfpfJCSW2pPVpjAPe3PXnNrY4uxtXFWJZWhSUTUVBOUf4K/cfvptuJ8MB2IXvjrvblNTbagABpsNySJeR1yKLmDqODUhnnYH3nfx2r7cP6Oj9/0a21yEBSOt2yfqT29nMin0FSl9Owm25BMefYdMsP3hHXie1EuGuKn4M8PTEGo4vD5DjFmIGYQzfc6QoOywvEZq15u2m1wN/fk93V3aU1qdowPfnzHr5jRJgL9l9z2bGNl6XNSrtl3DClbG9d2SwGFH4qxS1XyJK01bl9tp3wg3ABMrwDy0hytc9QASjBkHRxMhUOOQ2UIgtP8GPqQOh3rvothf2J856/f/TdMhOq8/e5ffhn9SI17wLPoMqLZZbdsLdWzgg7D1TCzEZqP+EHwangC/5QNV5ytgFsZlA0+AJmg09PylyJbZY9pCnmYYV0mwXvG2xjfU32o6V7qGTlwHP5+/uUHf9tx1C60a7Ih0GTRUkZkyOoZzWpQNzeokxvZCIJNcy7zNfj+44C3UiSytk+YaRwFOCH6SEo+TMIDVG2BpWkLPgyBB4Bsg5ljJLtGV3b6Jlfx34ROdbSOW+Mb1l6tjltHa2KcAs4jhOfULU9Bs4GNczTbxHYZ0/HJ7N/6z8DPPULhY935UHuq6paGKQtn3Xk+97mXCxx3EwDgkZFsY4pim7LRm1CVzxbsR9TeTjuBDiDQG9SmzoE1DvHrcBGY++FnGV7Ia103MXQf1j0zf9t+gDwnfifSbDVEHrUXHCIPmsRY5MvnEqDi/zIBwBpuI3OMHlmJ6hbZD/Xa5CNujI6t4VbFaGJ03wKKu245a/sfHXf8hXwTmtgDTWjrBp9n8P+E+ZzGfwykuQWOxDEciTZ4FOeJtshX4jBkyEs7X1mn+lZN8svyv4W/N/7e+Htr3/c/160VAIzxF//5l+jvjW/BTehVHqzrGsmRHEMjM5LnPaf3o25qacLWQLLwP4N/f/snzBf+reANdOMYHANH+hhIz3HLkY5koz+TRzqNRqRJeGrUpqY0XJb/FvmW/KXxe+eecfjWjv92zrb+pO0vvf3S9a3hLeb4aJHCYY3UMdQGxy6GNW/BXB1F/TQhxBbz+RN4P4JnXvLYrGeKj0Jew5UxDWHfcSQOTUP4HDimdsYEnJAtzgwta5Y9GfQtxlvqW2pP753DNFqXsHa1vTf9ZeMvDe+Bm6aSUQWswZEcqUwdRygcgz7o/jQhECK4tTkZby/32OB2u772yfl8Pc2exzH3EY2BTBwDY6CpvID7gUOW8Hlwfq7urCHwUGaVRtYje/pNucc4jIQMityks8l/a/yl4U8Nb+G38E1ua4trGsMazpHMZA7mYN1xc69BSusuhy/kX4B/eevhRrxl/I6Yke84kEY7kMbnjjbwyTNfG8sjm3C4DP66mOc4qCtvMQ5zuDZtCWAjT0F2E94D35rf5G3pwhXqh3kkW5blRxvpTHt6/ty3NS3/gjziucJD/3HAO8nH6fzp3jEOHMFMiPg8QEH7+hujiYes/HKkwjXhbMouvUWe275JbZrg0+hCEb7JtyXdcDLPsvw9FZkjNZIxrmdZyJsm8t7Q4inmNbQvihy/BEae94U7cQR6+hiQeCKvi6oNpS0xt9jU/QC47iwPZYvRk2PxoDkdw0dMvS3BNjdFYO0OOEM9RYzkIEdqH9FG9kEk/TSX4XL1enyhufkLs5+rN59UECrYAI+BSIxjvn8fRyZa3Q6la0Xh0w/zZLg6E/e47rRFp7fkEqHuuoXWTpHT7NM4wLBGeogjmUM5iFzBFZeAL07yv/D5L/+xQjF/fcT0f0diHDxm4Trxlzm0gSOow+JTRbJc58TvULbzJRxUT20pz0xGXvRJrifzQkwAI7lDGh6hNJ18SvW8+nw0v9h4e039d8xa0lkt0On8h8wDEnEmwo5jYARaosUy+Dtym84H5DaBMaWejJ65p4Zp81jy24L6gLleDfMwuRJ+Ju2Lc807jnQ+uoz/D4G/wtZjbVRxTsLnJ1qb+2TTbDHJX+fjstHzn6T4avlKkqRJ1Q6PbqU58tzhNZ8sTZoijiSm8odhyjxK6kmlac+bKxeQZfPRJornaP8l7Y/Ir/GPAFKQ12769aW8LtljFZ30kwRQmhBKSUmbJOum8RBHyl7GD5pAEvR12JZzX3JlSnqZ/fUEz3CKlQfwPzL79dxX5injoAQnWpthrzaRnDZ2PfdFQnXeb2DI+i5M/Hbtn3GxV/+9wKaSwzoSJOD6lyUTc/17iwZ67c7+4T/a+aqfW+B/9IE7k+doTvMjmZN8Xmzp9DHerass8BqHrp4PMOjyimm0wEjaDHOkACETEnIav+jasmXTnGVkfcR+dPsLAXOp9dzb/FTAe/jD+1/ebeHO8x/+nnlVXvjPyWwJr3cFuv6ARE2r+FUbf04HfnImfFW2zF/+ly76/7Pj3+D/VY9/afD/ByrJyOr8A2ohAAAAAElFTkSuQmCC\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.3773, 4.3773, 4.3773,  ..., 5.2641, 5.2641, 5.2641],\n",
      "        [4.3773, 4.3773, 4.3773,  ..., 5.2641, 5.2641, 5.2641],\n",
      "        [4.3773, 4.3773, 4.3773,  ..., 5.2641, 5.2641, 5.2641],\n",
      "        ...,\n",
      "        [2.5573, 2.5573, 2.5573,  ..., 2.6866, 2.6866, 2.6866],\n",
      "        [2.5573, 2.5573, 2.5573,  ..., 2.6866, 2.6866, 2.6866],\n",
      "        [2.5573, 2.5573, 2.5573,  ..., 2.6866, 2.6866, 2.6866]])\n"
     ]
    }
   ],
   "source": [
    "image_tensor = np_camera_image\n",
    "transform = make_depth_transform() # Apply the transformation pipeline\n",
    "\n",
    "transformed_image = transform(image_tensor)\n",
    "# batch = transformed_image.unsqueeze(0).cuda() # Make a batch of one image\n",
    "batch = transformed_image.unsqueeze(0) # Make a batch of one image\n",
    "\n",
    "with torch.inference_mode():\n",
    "    result = model.whole_inference(batch, img_meta=None, rescale=True)\n",
    "\n",
    "# Question why is result 3d??\n",
    "treated_image_2d = result.squeeze().cpu()\n",
    "depth_image = render_depth(result.squeeze().cpu())\n",
    "display(depth_image)\n",
    "print(result.squeeze().cpu())\n",
    "# treated_image = treated_image_2d.flatten()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "action_state = decision_steps.obs[1][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Image  7071\n",
      "[ 46.452003  46.452003  46.452003 ... -30.26       3.42      -0.67    ]\n"
     ]
    }
   ],
   "source": [
    "flattened_image = treated_image_2d.flatten()\n",
    "\n",
    "flattened_image\n",
    "state = np.concatenate((flattened_image,action_state ), 0)\n",
    "print('Length of Image ', len(state))\n",
    "print(state)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = decision_steps.obs[0][0]  # Agent's Camera view\n",
    "y = decision_steps.obs[1][0]  # Agent's position (x,y,z) + Target's position (x,y,z)\n",
    "state = np.concatenate((x, y), 0)\n",
    "state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env.reset()\n",
    "while True:\n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    x = decision_steps.obs[0][0] # Ray Perception 3D\n",
    "    y = decision_steps.obs[1][0] # Agent's camera\n",
    "    print(y)\n",
    "    action = ActionTuple(np.array([0, 0, 1], dtype = np.float32)) # for testing 0,0,1 simultaneuosly\n",
    "    env.set_actions(behavior_name, action)\n",
    "    # move the agent along with the action.\n",
    "    env.step()\n",
    "    action = action._continuous # converting ActionTuple to array\n",
    "    next_decision_steps, next_terminal_steps = env.get_steps(behavior_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "env.close()\n",
    "# TODO: OpenCV!!!\n",
    "# Combine the two SAC + DINO2\n",
    "# 256 x 256 looking good"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_states()\n",
    "\n",
    "agent_target_states = [...]\n",
    "np_camera_image = [[...]]\n",
    "\n",
    "treated_image = dinov2(np_camera_image) # 384\n",
    "\n",
    "states_for_training = np.concat(agent_target_states, treated_image)\n",
    "\n",
    "SAC same\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
