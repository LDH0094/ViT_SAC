{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Meta's Dept Estimation load"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dinov2.eval.depth.models import build_depther\n",
    "import urllib\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "class CenterPadding(torch.nn.Module):\n",
    "    def __init__(self, multiple):\n",
    "        super().__init__()\n",
    "        self.multiple = multiple\n",
    "\n",
    "    def _get_pad(self, size):\n",
    "        new_size = math.ceil(size / self.multiple) * self.multiple\n",
    "        pad_size = new_size - size\n",
    "        pad_size_left = pad_size // 2\n",
    "        pad_size_right = pad_size - pad_size_left\n",
    "        return pad_size_left, pad_size_right\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, x):\n",
    "        pads = list(itertools.chain.from_iterable(self._get_pad(m) for m in x.shape[:1:-1]))\n",
    "        output = F.pad(x, pads)\n",
    "        return output\n",
    "\n",
    "\n",
    "def create_depther(cfg, backbone_model, backbone_size, head_type):\n",
    "    train_cfg = cfg.get(\"train_cfg\")\n",
    "    test_cfg = cfg.get(\"test_cfg\")\n",
    "    depther = build_depther(cfg.model, train_cfg=train_cfg, test_cfg=test_cfg)\n",
    "\n",
    "    depther.backbone.forward = partial(\n",
    "        backbone_model.get_intermediate_layers,\n",
    "        n=cfg.model.backbone.out_indices,\n",
    "        reshape=True,\n",
    "        return_class_token=cfg.model.backbone.output_cls_token,\n",
    "        norm=cfg.model.backbone.final_norm,\n",
    "    )\n",
    "\n",
    "    if hasattr(backbone_model, \"patch_size\"):\n",
    "        depther.backbone.register_forward_pre_hook(lambda _, x: CenterPadding(backbone_model.patch_size)(x[0]))\n",
    "\n",
    "    return depther"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/ideoghyeon/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/Users/ideoghyeon/Library/CloudStorage/OneDrive-ateneo.edu/Thesis/Lee_RL/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/Users/ideoghyeon/Library/CloudStorage/OneDrive-ateneo.edu/Thesis/Lee_RL/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/Users/ideoghyeon/Library/CloudStorage/OneDrive-ateneo.edu/Thesis/Lee_RL/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "DinoVisionTransformer(\n  (patch_embed): PatchEmbed(\n    (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n    (norm): Identity()\n  )\n  (blocks): ModuleList(\n    (0-11): 12 x NestedTensorBlock(\n      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (attn): MemEffAttention(\n        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): LayerScale()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n        (act): GELU(approximate='none')\n        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n        (drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): LayerScale()\n      (drop_path2): Identity()\n    )\n  )\n  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n  (head): Identity()\n)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BACKBONE_SIZE = \"small\" # in (\"small\", \"base\", \"large\" or \"giant\")\n",
    "\n",
    "\n",
    "backbone_archs = {\n",
    "    \"small\": \"vits14\",\n",
    "    \"base\": \"vitb14\",\n",
    "    \"large\": \"vitl14\",\n",
    "    \"giant\": \"vitg14\",\n",
    "}\n",
    "backbone_arch = backbone_archs[BACKBONE_SIZE]\n",
    "backbone_name = f\"dinov2_{backbone_arch}\"\n",
    "\n",
    "backbone_model = torch.hub.load(repo_or_dir=\"facebookresearch/dinov2\", model=backbone_name)\n",
    "backbone_model.to(\"cpu\")\n",
    "backbone_model.eval()\n",
    "# backbone_model.cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from http path: https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_kitti_linear_head.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": "DepthEncoderDecoder(\n  (backbone): DinoVisionTransformer()\n  (decode_head): BNHead(\n    align_corners=False\n    (loss_decode): ModuleList(\n      (0): SigLoss()\n      (1): GradientLoss()\n    )\n    (softmax): Softmax(dim=1)\n    (conv_depth): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n    (relu): ReLU()\n    (sigmoid): Sigmoid()\n  )\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "\n",
    "import mmcv\n",
    "from mmcv.runner import load_checkpoint\n",
    "\n",
    "\n",
    "def load_config_from_url(url: str) -> str:\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        return f.read().decode()\n",
    "\n",
    "\n",
    "HEAD_DATASET = \"kitti\" # in (\"nyu\", \"kitti\")\n",
    "HEAD_TYPE = \"linear\" # in (\"linear\", \"linear4\", \"dpt\")\n",
    "\n",
    "\n",
    "DINOV2_BASE_URL = \"https://dl.fbaipublicfiles.com/dinov2\"\n",
    "head_config_url = f\"{DINOV2_BASE_URL}/{backbone_name}/{backbone_name}_{HEAD_DATASET}_{HEAD_TYPE}_config.py\"\n",
    "head_checkpoint_url = f\"{DINOV2_BASE_URL}/{backbone_name}/{backbone_name}_{HEAD_DATASET}_{HEAD_TYPE}_head.pth\"\n",
    "\n",
    "cfg_str = load_config_from_url(head_config_url)\n",
    "cfg = mmcv.Config.fromstring(cfg_str, file_format=\".py\")\n",
    "\n",
    "model = create_depther(\n",
    "    cfg,\n",
    "    backbone_model=backbone_model,\n",
    "    backbone_size=BACKBONE_SIZE,\n",
    "    head_type=HEAD_TYPE,\n",
    ")\n",
    "\n",
    "load_checkpoint(model, head_checkpoint_url, map_location=\"cpu\")\n",
    "model.eval()\n",
    "# model.cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building Pipeline for transform"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def make_depth_transform() -> transforms.Compose:\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        lambda x: 255.0 * x[:3], # Discard alpha component and scale by 255\n",
    "        transforms.Normalize(\n",
    "            mean=(123.675, 116.28, 103.53),\n",
    "            std=(58.395, 57.12, 57.375),\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "\n",
    "def render_depth(values, colormap_name=\"magma_r\") -> Image:\n",
    "    min_value, max_value = values.min(), values.max()\n",
    "    normalized_values = (values - min_value) / (max_value - min_value)\n",
    "\n",
    "    colormap = matplotlib.colormaps[colormap_name]\n",
    "    colors = colormap(normalized_values, bytes=True) # ((1)xhxwx4)\n",
    "    colors = colors[:, :, :3] # Discard alpha component\n",
    "    return Image.fromarray(colors)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Drone Enviornment Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "from collections import deque\n",
    "from replaybuffer import ReplayBuffers\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, action_dim, state_dim):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "        self.std_bound = [1e-2, 1.0]\n",
    "\n",
    "        self.h1 = nn.Linear(state_dim, 128)\n",
    "        self.h2 = nn.Linear(128, 64)\n",
    "        self.h3 = nn.Linear(64, 32)\n",
    "        self.h4 = nn.Linear(32, 16)\n",
    "        self.mu = nn.Linear(16, action_dim)\n",
    "        self.std = nn.Linear(16, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = nn.functional.relu(self.h1(state))\n",
    "        x = nn.functional.relu(self.h2(x))\n",
    "        x = nn.functional.relu(self.h3(x))\n",
    "        x = nn.functional.relu(self.h4(x))\n",
    "        mu = torch.tanh(self.mu(x))\n",
    "        std = nn.functional.softplus(self.std(x))\n",
    "\n",
    "        std = torch.clamp(std, self.std_bound[0], self.std_bound[1])\n",
    "\n",
    "        return mu, std\n",
    "\n",
    "    def sample_normal(self, mu, std):\n",
    "        normal_prob = Normal(mu, std)\n",
    "        action = normal_prob.sample()\n",
    "\n",
    "        # limit the action value\n",
    "        log_prob = normal_prob.log_prob(action)\n",
    "        log_prob = torch.sum(log_prob, dim=1, keepdim=True)\n",
    "\n",
    "        return action, log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, action_dim, state_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.x1 = nn.Linear(state_dim, 128)\n",
    "        self.a1 = nn.Linear(action_dim, 128)\n",
    "        # this layer is responsible for taking mixed state_action len.\n",
    "        self.h = nn.Sequential(\n",
    "            # 256 because it will be connected to two input tensors\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, state_action):\n",
    "        state = state_action[0]\n",
    "        action = state_action[1]\n",
    "        x = nn.functional.relu(self.x1(state))\n",
    "        a = nn.functional.relu(self.a1(action))\n",
    "        h = torch.cat((x, a), dim=-1)\n",
    "        q = self.h(h)\n",
    "        return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SACagent(object):\n",
    "    def __init__(self, N_STATES, N_ACTIONS):\n",
    "        # Hyperparameters\n",
    "        self.GAMMA = 0.99\n",
    "        self.BATCH_SIZE = 1024\n",
    "        self.BUFFER_SIZE = 100000\n",
    "        self.ACTOR_LEARNING_RATE = 0.0003\n",
    "        self.CRITIC_LEARNING_RATE = 0.001\n",
    "        self.TAU = 0.001\n",
    "        self.ALPHA = 0.5\n",
    "\n",
    "        # Observation space and Action space\n",
    "        self.state_dim = N_STATES\n",
    "        self.action_dim = N_ACTIONS\n",
    "\n",
    "        # Check if CUDA is available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Build Actor, Q1, Q2 and their target networks\n",
    "        self.actor = Actor(action_dim=self.action_dim, state_dim=self.state_dim)\n",
    "\n",
    "\n",
    "        self.critic_1 = Critic(action_dim=self.action_dim, state_dim=self.state_dim)\n",
    "        self.target_critic_1 = Critic(action_dim=self.action_dim, state_dim=self.state_dim)\n",
    "\n",
    "        self.critic_2 = Critic(action_dim=self.action_dim, state_dim=self.state_dim)\n",
    "        self.target_critic_2 = Critic(action_dim=self.action_dim, state_dim=self.state_dim)\n",
    "\n",
    "        # self.target_critic_1.load_state_dict(self.critic_1.state_dict())\n",
    "        # self.target_critic_2.load_state_dict(self.critic_2.state_dict())\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.ACTOR_LEARNING_RATE)\n",
    "        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=self.CRITIC_LEARNING_RATE)\n",
    "        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=self.CRITIC_LEARNING_RATE)\n",
    "\n",
    "        # Clear out the buffer\n",
    "        self.buffer = ReplayBuffers(self.BUFFER_SIZE)\n",
    "\n",
    "        # For plotting purposes, data is stored.\n",
    "        self.policy_loss = []\n",
    "        self.reward_list = []\n",
    "\n",
    "    def get_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor([state], dtype=torch.float32).to(self.device)\n",
    "            mu, std = self.actor(state)\n",
    "            normal = torch.distributions.Normal(mu, std)\n",
    "            action = normal.sample()\n",
    "            return action.cpu().numpy()\n",
    "\n",
    "    def update_target_network(self, TAU):\n",
    "        phi_1 = self.critic_1.state_dict()\n",
    "        phi_2 = self.critic_2.state_dict()\n",
    "        target_phi_1 = self.target_critic_1.state_dict()\n",
    "        target_phi_2 = self.target_critic_2.state_dict()\n",
    "        for name in phi_1:\n",
    "            target_phi_1[name] = TAU * phi_1[name] + (1 - TAU) * target_phi_1[name]\n",
    "            target_phi_2[name] = TAU * phi_2[name] + (1 - TAU) * target_phi_2[name]\n",
    "        self.target_critic_1.load_state_dict(target_phi_1)\n",
    "        self.target_critic_2.load_state_dict(target_phi_2)\n",
    "\n",
    "\n",
    "    # train Q1, Q2\n",
    "    def critic_learn(self, states, actions, y_i):\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32).to(self.device)\n",
    "        q_targets = y_i.to(self.device)\n",
    "        \n",
    "        q_1 = self.critic_1([states, actions])\n",
    "        # where q_1 is the predicted value, q_targets is the true val.\n",
    "        loss_1 = F.mse_loss(q_1, q_targets)\n",
    "\n",
    "        # sets the gradients of all parameters of the optimizer to zero. This is necessary to prevent the gradients from accumulating from multiple backpropagation passes.\n",
    "        self.critic_1_optimizer.zero_grad()\n",
    "        #computes the gradients of the loss with respect to all the learnable parameters in the critic network\n",
    "        loss_1.backward()\n",
    "        #updates the parameters of the critic network based on the computed gradients.\n",
    "        self.critic_1_optimizer.step()\n",
    "\n",
    "        q_2 = self.critic_2([states, actions])\n",
    "        loss_2 = F.mse_loss(q_2, q_targets)\n",
    "\n",
    "        self.critic_2_optimizer.zero_grad()\n",
    "        loss_2.backward()\n",
    "        self.critic_2_optimizer.step()\n",
    "\n",
    "    def actor_learn(self, states):\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        mu, std = self.actor(states)\n",
    "        actions, log_pdfs = self.actor.sample_normal(mu, std)\n",
    "        log_pdfs = log_pdfs.squeeze(1)\n",
    "        soft_q_1 = self.critic_1([states, actions])\n",
    "        soft_q_2 = self.critic_2([states, actions])\n",
    "        soft_q = torch.min(soft_q_1, soft_q_2)\n",
    "\n",
    "        loss = torch.mean(self.ALPHA * log_pdfs - soft_q)\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        return float(loss)\n",
    "\n",
    "    def q_target(self, rewards, q_values, dones):\n",
    "        y_k = np.asarray(q_values)\n",
    "        for i in range(q_values.shape[0]):\n",
    "            if dones[i]:\n",
    "                y_k[i] = rewards[i]\n",
    "            else:\n",
    "                y_k[i] = rewards[i] + self.GAMMA * q_values[i]\n",
    "        return torch.tensor(y_k, dtype=torch.float32)\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        self.actor.load_state_dict(torch.load(path + 'Drone_actor_2q.pth'))\n",
    "        self.critic_1.load_state_dict(torch.load(path + 'Drone_critic_12q.pth'))\n",
    "        self.critic_2.load_state_dict(torch.load(path + 'Drone_critic_22q.pth'))\n",
    "\n",
    "\n",
    "    def train(self, max_episode_num, env, behavior_name):\n",
    "        transform = make_depth_transform() # Apply the transformation pipeline\n",
    "\n",
    "        self.param_counter = 0\n",
    "        cnt = 0\n",
    "        # reset target network param.\n",
    "        self.update_target_network(1.0)\n",
    "\n",
    "        for ep in range(int(max_episode_num)):\n",
    "            is_learning = False\n",
    "            frame, episode_reward = 0, 0\n",
    "            # reset the enviornment\n",
    "            env.reset()\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            episode_done = False\n",
    "\n",
    "            # setting up the initial state as an array\n",
    "            #----------------------------------------\n",
    "            np_camera_image = decision_steps.obs[0][0] # Camera Sensor\n",
    "            v_agent_target = decision_steps.obs[1][0] # Agent's position (x,y,z) + Target's position (x,y,z)\n",
    "            image_tensor = np_camera_image     # just a naming for better understanding\n",
    "            transformed_image = transform(image_tensor)\n",
    "            # batch = transformed_image.unsqueeze(0).cuda() # Make a batch of one image\n",
    "            batch = transformed_image.unsqueeze(0) # Make a batch of one image\n",
    "            with torch.inference_mode():\n",
    "                result = model.whole_inference(batch, img_meta=None, rescale=True)\n",
    "            treated_image_2d = result.squeeze().cpu()\n",
    "            treated_image = treated_image_2d.flatten() # converts 2d into 1d\n",
    "            # now concat the two 1d array for state -> SAC\n",
    "            state = np.concatenate((treated_image, v_agent_target), 0)\n",
    "            #-----------------------------------------\n",
    "\n",
    "            while not episode_done:\n",
    "\n",
    "                action = self.get_action(state)\n",
    "                # wrap the action with ActionTuple before sending it to UE.\n",
    "                action = ActionTuple(np.array(action, dtype = np.float32))\n",
    "\n",
    "                env.set_actions(behavior_name, action)\n",
    "                # move the agent along with the action.\n",
    "                env.step()\n",
    "                action = action._continuous # converting ActionTuple to array\n",
    "                next_decision_steps, next_terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "                # if the agent is still on, collect data and add it to buffer.\n",
    "                if next_decision_steps:\n",
    "                    # get the reward.\n",
    "                    train_reward = next_decision_steps.reward[0]\n",
    "\n",
    "                    #----------------------------------------\n",
    "                    np_camera_image = decision_steps.obs[0][0] # Camera Sensor\n",
    "                    v_agent_target = decision_steps.obs[1][0] # Agent's position (x,y,z) + Target's position (x,y,z)\n",
    "                    image_tensor = np_camera_image     # just a naming for better understanding\n",
    "                    transformed_image = transform(image_tensor)\n",
    "                    # batch = transformed_image.unsqueeze(0).cuda() # Make a batch of one image\n",
    "                    batch = transformed_image.unsqueeze(0) # Make a batch of one image\n",
    "                    with torch.inference_mode():\n",
    "                        result = model.whole_inference(batch, img_meta=None, rescale=True)\n",
    "                    treated_image_2d = result.squeeze().cpu()\n",
    "                    treated_image = treated_image_2d.flatten() # converts 2d into 1d\n",
    "                    # now concat the two 1d array for state -> SAC\n",
    "                    next_state = np.concatenate((treated_image, v_agent_target), 0)\n",
    "                    #-----------------------------------------\n",
    "\n",
    "                    episode_reward += next_decision_steps.reward[0]\n",
    "                    # store the data to the buffer\n",
    "                    self.buffer.add_data(state, action, train_reward, next_state, False)\n",
    "                    episode_done = False\n",
    "\n",
    "                # if the agent is off, collect data and add True for done.\n",
    "                if next_terminal_steps:\n",
    "                    # get the reward.\n",
    "                    train_reward = next_terminal_steps.reward[0]\n",
    "\n",
    "                    #----------------------------------------\n",
    "                    np_camera_image = decision_steps.obs[0][0] # Camera Sensor\n",
    "                    v_agent_target = decision_steps.obs[1][0] # Agent's position (x,y,z) + Target's position (x,y,z)\n",
    "                    image_tensor = np_camera_image     # just a naming for better understanding\n",
    "                    transformed_image = transform(image_tensor)\n",
    "                    # batch = transformed_image.unsqueeze(0).cuda() # Make a batch of one image\n",
    "                    batch = transformed_image.unsqueeze(0) # Make a batch of one image\n",
    "                    with torch.inference_mode():\n",
    "                        result = model.whole_inference(batch, img_meta=None, rescale=True)\n",
    "                    treated_image_2d = result.squeeze().cpu()\n",
    "                    treated_image = treated_image_2d.flatten() # converts 2d into 1d\n",
    "                    # now concat the two 1d array for state -> SAC\n",
    "                    next_state = np.concatenate((treated_image, v_agent_target), 0)\n",
    "                    #-----------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "                    episode_reward += next_terminal_steps.reward[0]\n",
    "                    # store the data to the buffer\n",
    "                    self.buffer.add_data(state, action, train_reward, next_state, True)\n",
    "                    episode_done = True\n",
    "\n",
    "                # if buffer has enough data start training.\n",
    "                if self.buffer.buffer_count() > self.BUFFER_SIZE - self.BATCH_SIZE:\n",
    "                    is_learning = True\n",
    "\n",
    "                    states, actions, rewards, next_states, dones = self.buffer.sample_batch(self.BATCH_SIZE)\n",
    "\n",
    "                    # Calculate the Q target value\n",
    "                    with torch.no_grad():\n",
    "                        next_mu, next_std = self.actor(torch.tensor(next_states, dtype=torch.float32).to(self.device))\n",
    "                        next_actions, next_log_pdf = self.actor.sample_normal(next_mu, next_std)\n",
    "\n",
    "                        # convert np to tensor\n",
    "                        tensor_next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "                        tensor_next_actions = torch.tensor(next_actions, dtype=torch.float32)\n",
    "\n",
    "                        # move to CUDA\n",
    "                        tensor_next_states = tensor_next_states.to(self.device)\n",
    "                        tensor_next_actions = tensor_next_actions.to(self.device)\n",
    "\n",
    "                        target_qs_1 = self.target_critic_1([tensor_next_states, tensor_next_actions])\n",
    "                        target_qs_2 = self.target_critic_2([tensor_next_states, tensor_next_actions])\n",
    "                        target_qs = torch.min(target_qs_1, target_qs_2)\n",
    "\n",
    "                        target_qi = target_qs - self.ALPHA * next_log_pdf\n",
    "                        y_i = self.q_target(rewards, target_qi.numpy(), dones)\n",
    "\n",
    "\n",
    "                    self.critic_learn(states, actions, y_i)\n",
    "\n",
    "                    # update Actor and return policy loss\n",
    "                    policy_loss = self.actor_learn(states)\n",
    "\n",
    "                    self.update_target_network(self.TAU)\n",
    "\n",
    "                state = next_state\n",
    "                frame += 1\n",
    "                cnt += 1\n",
    "\n",
    "            # Episode output\n",
    "            print('Episode: ', ep+1, 'Frame: ', frame, 'u Reward: ', episode_reward)\n",
    "\n",
    "            if is_learning:\n",
    "                self.reward_list.append(train_reward)\n",
    "                self.policy_loss.append(policy_loss)\n",
    "\n",
    "            # every 250th run will store another params\n",
    "            if ep % 250 == 0:\n",
    "                torch.save(self.actor.state_dict(), \"./saved_weights/250th/actor_\"+ str(self.param_counter)+\"_2q.pth\")\n",
    "                torch.save(self.critic_1.state_dict(), \"./saved_weights/250th/critic_\"+str(self.param_counter)+\"_12q.pth\")\n",
    "                torch.save(self.critic_2.state_dict(), \"./saved_weights/250th/critic_\"+str(self.param_counter)+\"_22q.pth\")\n",
    "                self.param_counter += 1\n",
    "\n",
    "    def plot_result(self):\n",
    "        fig=plt.figure(figsize=(18, 6))\n",
    "        fig.add_subplot(1, 3, 1)  # 1 row, 3 columns\n",
    "        plt.plot(self.reward_list)\n",
    "\n",
    "        fig.add_subplot(1, 3, 3)\n",
    "        plt.plot(self.policy_loss)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Run Unity Enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_ACTIONS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# env = UnityEnvironment(file_name= \"./Linux_Drone_v0.0.2/Linux_Drone_with.x86_64\", base_port=5004)\n",
    "env = UnityEnvironment(file_name= None, base_port=5004)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "behavior_names = list(env.behavior_specs.keys())\n",
    "behavior_name = behavior_names[0]\n",
    "decision_steps, terminal_steps = env.get_steps(behavior_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "7071"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GET N_STATES or len_STATES rather\n",
    "transform = make_depth_transform() # Apply the transformation pipeline\n",
    "#----------------------------------------\n",
    "np_camera_image = decision_steps.obs[0][0] # Camera Sensor\n",
    "v_agent_target = decision_steps.obs[1][0] # Agent's position (x,y,z) + Target's position (x,y,z)\n",
    "image_tensor = np_camera_image     # just a naming for better understanding\n",
    "transformed_image = transform(image_tensor)\n",
    "# batch = transformed_image.unsqueeze(0).cuda() # Make a batch of one image\n",
    "batch = transformed_image.unsqueeze(0) # Make a batch of one image\n",
    "with torch.inference_mode():\n",
    "    result = model.whole_inference(batch, img_meta=None, rescale=True)\n",
    "treated_image_2d = result.squeeze().cpu()\n",
    "treated_image = treated_image_2d.flatten() # converts 2d into 1d\n",
    "# now concat the two 1d array for state -> SAC\n",
    "state = np.concatenate((treated_image, v_agent_target), 0)\n",
    "#-----------------------------------------\n",
    "N_STATES = len(state)\n",
    "N_STATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bx/11k7rc0x5xl576ckk4jkh6ch0000gn/T/ipykernel_15996/2415218324.py:45: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  state = torch.tensor([state], dtype=torch.float32).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1 Frame:  245 u Reward:  0.4626941680908203\n",
      "Episode:  2 Frame:  235 u Reward:  -3.06948184967041\n",
      "Episode:  3 Frame:  253 u Reward:  -3.3377685546875\n",
      "Episode:  4 Frame:  250 u Reward:  -2.6122045516967773\n",
      "Episode:  5 Frame:  229 u Reward:  -2.6375932693481445\n",
      "Episode:  6 Frame:  225 u Reward:  -0.2606182098388672\n",
      "Episode:  7 Frame:  394 u Reward:  -1.0883893966674805\n",
      "Episode:  8 Frame:  491 u Reward:  -2.5144147872924805\n",
      "Episode:  9 Frame:  195 u Reward:  -4.608125686645508\n",
      "Episode:  10 Frame:  203 u Reward:  -3.212590217590332\n",
      "Episode:  11 Frame:  320 u Reward:  1.0821256637573242\n",
      "Episode:  12 Frame:  215 u Reward:  2.100632667541504\n",
      "Episode:  13 Frame:  198 u Reward:  0.9745769500732422\n",
      "Episode:  14 Frame:  277 u Reward:  0.9189214706420898\n",
      "Episode:  15 Frame:  535 u Reward:  -2.9273834228515625\n",
      "Episode:  16 Frame:  195 u Reward:  -3.833804130554199\n",
      "Episode:  17 Frame:  242 u Reward:  3.6413278579711914\n",
      "Episode:  18 Frame:  495 u Reward:  -1.2835826873779297\n",
      "Episode:  19 Frame:  197 u Reward:  -5.636551856994629\n",
      "Episode:  20 Frame:  262 u Reward:  -7.365362167358398\n",
      "Episode:  21 Frame:  606 u Reward:  -2.258244514465332\n",
      "Episode:  22 Frame:  180 u Reward:  -6.069464683532715\n",
      "Episode:  23 Frame:  358 u Reward:  -2.6283206939697266\n",
      "Episode:  24 Frame:  432 u Reward:  -4.255913734436035\n",
      "Episode:  25 Frame:  197 u Reward:  -3.7694711685180664\n",
      "Episode:  26 Frame:  259 u Reward:  0.7977085113525391\n",
      "Episode:  27 Frame:  223 u Reward:  -3.204050064086914\n",
      "Episode:  28 Frame:  180 u Reward:  -4.558927536010742\n",
      "Episode:  29 Frame:  187 u Reward:  -3.486372947692871\n",
      "Episode:  30 Frame:  177 u Reward:  -2.7911643981933594\n",
      "Episode:  31 Frame:  234 u Reward:  -0.8046760559082031\n",
      "Episode:  32 Frame:  393 u Reward:  0.9199886322021484\n",
      "Episode:  33 Frame:  1000 u Reward:  -9.20921516418457\n",
      "Episode:  34 Frame:  185 u Reward:  -0.7370185852050781\n",
      "Episode:  35 Frame:  226 u Reward:  -1.3268098831176758\n",
      "Episode:  36 Frame:  335 u Reward:  0.6983509063720703\n",
      "Episode:  37 Frame:  163 u Reward:  -9.095548629760742\n",
      "Episode:  38 Frame:  246 u Reward:  -5.666872978210449\n",
      "Episode:  39 Frame:  219 u Reward:  -5.229914665222168\n",
      "Episode:  40 Frame:  539 u Reward:  -2.8834896087646484\n",
      "Episode:  41 Frame:  214 u Reward:  2.353189468383789\n",
      "Episode:  42 Frame:  214 u Reward:  -3.4254932403564453\n",
      "Episode:  43 Frame:  178 u Reward:  -2.7573671340942383\n",
      "Episode:  44 Frame:  323 u Reward:  -0.7571001052856445\n",
      "Episode:  45 Frame:  157 u Reward:  2.2337846755981445\n",
      "Episode:  46 Frame:  312 u Reward:  0.5234136581420898\n",
      "Episode:  47 Frame:  224 u Reward:  -3.6572275161743164\n",
      "Episode:  48 Frame:  338 u Reward:  -0.5479564666748047\n",
      "Episode:  49 Frame:  211 u Reward:  -4.452548980712891\n",
      "Episode:  50 Frame:  268 u Reward:  2.0313644409179688\n",
      "Episode:  51 Frame:  264 u Reward:  3.887576103210449\n",
      "Episode:  52 Frame:  371 u Reward:  0.6807498931884766\n",
      "Episode:  53 Frame:  496 u Reward:  -3.7561044692993164\n",
      "Episode:  54 Frame:  177 u Reward:  0.818699836730957\n",
      "Episode:  55 Frame:  278 u Reward:  -2.7201080322265625\n",
      "Episode:  56 Frame:  262 u Reward:  -3.2577943801879883\n",
      "Episode:  57 Frame:  311 u Reward:  0.639988899230957\n",
      "Episode:  58 Frame:  187 u Reward:  -5.988849639892578\n",
      "Episode:  59 Frame:  175 u Reward:  -5.256650924682617\n",
      "Episode:  60 Frame:  240 u Reward:  1.326857566833496\n",
      "Episode:  61 Frame:  535 u Reward:  -1.8999128341674805\n",
      "Episode:  62 Frame:  173 u Reward:  -4.403757095336914\n",
      "Episode:  63 Frame:  380 u Reward:  -0.5465087890625\n",
      "Episode:  64 Frame:  319 u Reward:  -2.6005687713623047\n",
      "Episode:  65 Frame:  226 u Reward:  -7.414680480957031\n",
      "Episode:  66 Frame:  219 u Reward:  -3.733837127685547\n",
      "Episode:  67 Frame:  361 u Reward:  0.2901344299316406\n",
      "Episode:  68 Frame:  459 u Reward:  -3.13192081451416\n",
      "Episode:  69 Frame:  222 u Reward:  0.6326360702514648\n",
      "Episode:  70 Frame:  207 u Reward:  -3.3220386505126953\n",
      "Episode:  71 Frame:  181 u Reward:  -5.6121416091918945\n"
     ]
    },
    {
     "ename": "UnityTimeOutException",
     "evalue": "The Unity environment took too long to respond. Make sure that :\n\t The environment does not need user interaction to launch\n\t The Agents' Behavior Parameters > Behavior Type is set to \"Default\"\n\t The environment and the Python interface have compatible versions.\n\t If you're running on a headless server without graphics support, turn off display by either passing --no-graphics option or build your Unity executable as server build.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUnityTimeOutException\u001B[0m                     Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m agent \u001B[38;5;241m=\u001B[39m SACagent(N_STATES, N_ACTIONS)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# usually 30K is enough.\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m30000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbehavior_name\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[9], line 159\u001B[0m, in \u001B[0;36mSACagent.train\u001B[0;34m(self, max_episode_num, env, behavior_name)\u001B[0m\n\u001B[1;32m    157\u001B[0m env\u001B[38;5;241m.\u001B[39mset_actions(behavior_name, action)\n\u001B[1;32m    158\u001B[0m \u001B[38;5;66;03m# move the agent along with the action.\u001B[39;00m\n\u001B[0;32m--> 159\u001B[0m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    160\u001B[0m action \u001B[38;5;241m=\u001B[39m action\u001B[38;5;241m.\u001B[39m_continuous \u001B[38;5;66;03m# converting ActionTuple to array\u001B[39;00m\n\u001B[1;32m    161\u001B[0m next_decision_steps, next_terminal_steps \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mget_steps(behavior_name)\n",
      "File \u001B[0;32m~/miniconda3/envs/rl2/lib/python3.9/site-packages/mlagents_envs/timers.py:305\u001B[0m, in \u001B[0;36mtimed.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m hierarchical_timer(func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m):\n\u001B[0;32m--> 305\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/rl2/lib/python3.9/site-packages/mlagents_envs/environment.py:348\u001B[0m, in \u001B[0;36mUnityEnvironment.step\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    346\u001B[0m step_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_step_input(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_env_actions)\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m hierarchical_timer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcommunicator.exchange\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 348\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_communicator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexchange\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll_process\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m outputs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    350\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m UnityCommunicatorStoppedException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCommunicator has exited.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/rl2/lib/python3.9/site-packages/mlagents_envs/rpc_communicator.py:142\u001B[0m, in \u001B[0;36mRpcCommunicator.exchange\u001B[0;34m(self, inputs, poll_callback)\u001B[0m\n\u001B[1;32m    140\u001B[0m message\u001B[38;5;241m.\u001B[39munity_input\u001B[38;5;241m.\u001B[39mCopyFrom(inputs)\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munity_to_external\u001B[38;5;241m.\u001B[39mparent_conn\u001B[38;5;241m.\u001B[39msend(message)\n\u001B[0;32m--> 142\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpoll_for_timeout\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpoll_callback\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    143\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munity_to_external\u001B[38;5;241m.\u001B[39mparent_conn\u001B[38;5;241m.\u001B[39mrecv()\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output\u001B[38;5;241m.\u001B[39mheader\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m200\u001B[39m:\n",
      "File \u001B[0;32m~/miniconda3/envs/rl2/lib/python3.9/site-packages/mlagents_envs/rpc_communicator.py:114\u001B[0m, in \u001B[0;36mRpcCommunicator.poll_for_timeout\u001B[0;34m(self, poll_callback)\u001B[0m\n\u001B[1;32m    111\u001B[0m         poll_callback()\n\u001B[1;32m    113\u001B[0m \u001B[38;5;66;03m# Got this far without reading any data from the connection, so it must be dead.\u001B[39;00m\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m UnityTimeOutException(\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe Unity environment took too long to respond. Make sure that :\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    116\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m The environment does not need user interaction to launch\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    117\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m The Agents\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124m Behavior Parameters > Behavior Type is set to \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDefault\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m The environment and the Python interface have compatible versions.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m If you\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mre running on a headless server without graphics support, turn off display \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mby either passing --no-graphics option or build your Unity executable as server build.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    121\u001B[0m )\n",
      "\u001B[0;31mUnityTimeOutException\u001B[0m: The Unity environment took too long to respond. Make sure that :\n\t The environment does not need user interaction to launch\n\t The Agents' Behavior Parameters > Behavior Type is set to \"Default\"\n\t The environment and the Python interface have compatible versions.\n\t If you're running on a headless server without graphics support, turn off display by either passing --no-graphics option or build your Unity executable as server build."
     ]
    }
   ],
   "source": [
    "agent = SACagent(N_STATES, N_ACTIONS)\n",
    "# usually 30K is enough.\n",
    "agent.train(30000, env, behavior_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent.plot_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
