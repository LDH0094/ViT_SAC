{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Meta's Dept Estimation load"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dinov2.eval.depth.models import build_depther\n",
    "import urllib\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "class CenterPadding(torch.nn.Module):\n",
    "    def __init__(self, multiple):\n",
    "        super().__init__()\n",
    "        self.multiple = multiple\n",
    "\n",
    "    def _get_pad(self, size):\n",
    "        new_size = math.ceil(size / self.multiple) * self.multiple\n",
    "        pad_size = new_size - size\n",
    "        pad_size_left = pad_size // 2\n",
    "        pad_size_right = pad_size - pad_size_left\n",
    "        return pad_size_left, pad_size_right\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, x):\n",
    "        pads = list(itertools.chain.from_iterable(self._get_pad(m) for m in x.shape[:1:-1]))\n",
    "        output = F.pad(x, pads)\n",
    "        return output\n",
    "\n",
    "\n",
    "def create_depther(cfg, backbone_model, backbone_size, head_type):\n",
    "    train_cfg = cfg.get(\"train_cfg\")\n",
    "    test_cfg = cfg.get(\"test_cfg\")\n",
    "    depther = build_depther(cfg.model, train_cfg=train_cfg, test_cfg=test_cfg)\n",
    "\n",
    "    depther.backbone.forward = partial(\n",
    "        backbone_model.get_intermediate_layers,\n",
    "        n=cfg.model.backbone.out_indices,\n",
    "        reshape=True,\n",
    "        return_class_token=cfg.model.backbone.output_cls_token,\n",
    "        norm=cfg.model.backbone.final_norm,\n",
    "    )\n",
    "\n",
    "    if hasattr(backbone_model, \"patch_size\"):\n",
    "        depther.backbone.register_forward_pre_hook(lambda _, x: CenterPadding(backbone_model.patch_size)(x[0]))\n",
    "\n",
    "    return depther"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/ideoghyeon/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "data": {
      "text/plain": "DinoVisionTransformer(\n  (patch_embed): PatchEmbed(\n    (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n    (norm): Identity()\n  )\n  (blocks): ModuleList(\n    (0-11): 12 x NestedTensorBlock(\n      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (attn): MemEffAttention(\n        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): LayerScale()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n        (act): GELU(approximate='none')\n        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n        (drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): LayerScale()\n      (drop_path2): Identity()\n    )\n  )\n  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n  (head): Identity()\n)"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BACKBONE_SIZE = \"small\" # in (\"small\", \"base\", \"large\" or \"giant\")\n",
    "\n",
    "\n",
    "backbone_archs = {\n",
    "    \"small\": \"vits14\",\n",
    "    \"base\": \"vitb14\",\n",
    "    \"large\": \"vitl14\",\n",
    "    \"giant\": \"vitg14\",\n",
    "}\n",
    "backbone_arch = backbone_archs[BACKBONE_SIZE]\n",
    "backbone_name = f\"dinov2_{backbone_arch}\"\n",
    "\n",
    "backbone_model = torch.hub.load(repo_or_dir=\"facebookresearch/dinov2\", model=backbone_name)\n",
    "backbone_model.to(\"cpu\")\n",
    "backbone_model.eval()\n",
    "# backbone_model.cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from http path: https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_kitti_linear_head.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": "DepthEncoderDecoder(\n  (backbone): DinoVisionTransformer()\n  (decode_head): BNHead(\n    align_corners=False\n    (loss_decode): ModuleList(\n      (0): SigLoss()\n      (1): GradientLoss()\n    )\n    (softmax): Softmax(dim=1)\n    (conv_depth): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n    (relu): ReLU()\n    (sigmoid): Sigmoid()\n  )\n)"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "\n",
    "import mmcv\n",
    "from mmcv.runner import load_checkpoint\n",
    "\n",
    "\n",
    "def load_config_from_url(url: str) -> str:\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        return f.read().decode()\n",
    "\n",
    "\n",
    "HEAD_DATASET = \"kitti\" # in (\"nyu\", \"kitti\")\n",
    "HEAD_TYPE = \"linear\" # in (\"linear\", \"linear4\", \"dpt\")\n",
    "\n",
    "\n",
    "DINOV2_BASE_URL = \"https://dl.fbaipublicfiles.com/dinov2\"\n",
    "head_config_url = f\"{DINOV2_BASE_URL}/{backbone_name}/{backbone_name}_{HEAD_DATASET}_{HEAD_TYPE}_config.py\"\n",
    "head_checkpoint_url = f\"{DINOV2_BASE_URL}/{backbone_name}/{backbone_name}_{HEAD_DATASET}_{HEAD_TYPE}_head.pth\"\n",
    "\n",
    "cfg_str = load_config_from_url(head_config_url)\n",
    "cfg = mmcv.Config.fromstring(cfg_str, file_format=\".py\")\n",
    "\n",
    "model = create_depther(\n",
    "    cfg,\n",
    "    backbone_model=backbone_model,\n",
    "    backbone_size=BACKBONE_SIZE,\n",
    "    head_type=HEAD_TYPE,\n",
    ")\n",
    "\n",
    "load_checkpoint(model, head_checkpoint_url, map_location=\"cpu\")\n",
    "model.eval()\n",
    "# model.cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building Pipeline for transform"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def make_depth_transform() -> transforms.Compose:\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        lambda x: 255.0 * x[:3], # Discard alpha component and scale by 255\n",
    "        transforms.Normalize(\n",
    "            mean=(123.675, 116.28, 103.53),\n",
    "            std=(58.395, 57.12, 57.375),\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "\n",
    "def render_depth(values, colormap_name=\"magma_r\") -> Image:\n",
    "    min_value, max_value = values.min(), values.max()\n",
    "    normalized_values = (values - min_value) / (max_value - min_value)\n",
    "\n",
    "    colormap = matplotlib.colormaps[colormap_name]\n",
    "    colors = colormap(normalized_values, bytes=True) # ((1)xhxwx4)\n",
    "    colors = colors[:, :, :3] # Discard alpha component\n",
    "    return Image.fromarray(colors)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Drone Enviornment Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "from collections import deque\n",
    "from replaybuffer import ReplayBuffers\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, action_dim, state_dim):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "        self.std_bound = [1e-2, 1.0]\n",
    "\n",
    "        self.h1 = nn.Linear(state_dim, 128)\n",
    "        self.h2 = nn.Linear(128, 64)\n",
    "        self.h3 = nn.Linear(64, 32)\n",
    "        self.h4 = nn.Linear(32, 16)\n",
    "        self.mu = nn.Linear(16, action_dim)\n",
    "        self.std = nn.Linear(16, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = nn.functional.relu(self.h1(state))\n",
    "        x = nn.functional.relu(self.h2(x))\n",
    "        x = nn.functional.relu(self.h3(x))\n",
    "        x = nn.functional.relu(self.h4(x))\n",
    "        mu = torch.tanh(self.mu(x))\n",
    "        std = nn.functional.softplus(self.std(x))\n",
    "\n",
    "        std = torch.clamp(std, self.std_bound[0], self.std_bound[1])\n",
    "\n",
    "        return mu, std\n",
    "\n",
    "    def sample_normal(self, mu, std):\n",
    "        normal_prob = Normal(mu, std)\n",
    "        action = normal_prob.sample()\n",
    "\n",
    "        # limit the action value\n",
    "        log_prob = normal_prob.log_prob(action)\n",
    "        log_prob = torch.sum(log_prob, dim=1, keepdim=True)\n",
    "\n",
    "        return action, log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, action_dim, state_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.x1 = nn.Linear(state_dim, 128)\n",
    "        self.a1 = nn.Linear(action_dim, 128)\n",
    "        # this layer is responsible for taking mixed state_action len.\n",
    "        self.h = nn.Sequential(\n",
    "            # 256 because it will be connected to two input tensors\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, state_action):\n",
    "        state = state_action[0]\n",
    "        action = state_action[1]\n",
    "        x = nn.functional.relu(self.x1(state))\n",
    "        a = nn.functional.relu(self.a1(action))\n",
    "        h = torch.cat((x, a), dim=-1)\n",
    "        q = self.h(h)\n",
    "        return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SACagent(object):\n",
    "    def __init__(self, N_STATES, N_ACTIONS):\n",
    "        # Hyperparameters\n",
    "        self.GAMMA = 0.99\n",
    "        self.BATCH_SIZE = 1024\n",
    "        self.BUFFER_SIZE = 1000000\n",
    "        self.ACTOR_LEARNING_RATE = 0.0003\n",
    "        self.CRITIC_LEARNING_RATE = 0.001\n",
    "        self.TAU = 0.001\n",
    "        self.ALPHA = 0.5\n",
    "\n",
    "        # Observation space and Action space\n",
    "        self.state_dim = N_STATES\n",
    "        self.action_dim = N_ACTIONS\n",
    "\n",
    "        # Check if CUDA is available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Build Actor, Q1, Q2 and their target networks\n",
    "        self.actor = Actor(action_dim=self.action_dim, state_dim=self.state_dim)\n",
    "\n",
    "\n",
    "        self.critic_1 = Critic(action_dim=self.action_dim, state_dim=self.state_dim)\n",
    "        self.target_critic_1 = Critic(action_dim=self.action_dim, state_dim=self.state_dim)\n",
    "\n",
    "        self.critic_2 = Critic(action_dim=self.action_dim, state_dim=self.state_dim)\n",
    "        self.target_critic_2 = Critic(action_dim=self.action_dim, state_dim=self.state_dim)\n",
    "\n",
    "        # self.target_critic_1.load_state_dict(self.critic_1.state_dict())\n",
    "        # self.target_critic_2.load_state_dict(self.critic_2.state_dict())\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.ACTOR_LEARNING_RATE)\n",
    "        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=self.CRITIC_LEARNING_RATE)\n",
    "        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=self.CRITIC_LEARNING_RATE)\n",
    "\n",
    "        # Clear out the buffer\n",
    "        self.buffer = ReplayBuffers(self.BUFFER_SIZE)\n",
    "\n",
    "        # For plotting purposes, data is stored.\n",
    "        self.policy_loss = []\n",
    "        self.reward_list = []\n",
    "\n",
    "    def get_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor([state], dtype=torch.float32).to(self.device)\n",
    "            mu, std = self.actor(state)\n",
    "            normal = torch.distributions.Normal(mu, std)\n",
    "            action = normal.sample()\n",
    "            return action.cpu().numpy()\n",
    "\n",
    "    def update_target_network(self, TAU):\n",
    "        phi_1 = self.critic_1.state_dict()\n",
    "        phi_2 = self.critic_2.state_dict()\n",
    "        target_phi_1 = self.target_critic_1.state_dict()\n",
    "        target_phi_2 = self.target_critic_2.state_dict()\n",
    "        for name in phi_1:\n",
    "            target_phi_1[name] = TAU * phi_1[name] + (1 - TAU) * target_phi_1[name]\n",
    "            target_phi_2[name] = TAU * phi_2[name] + (1 - TAU) * target_phi_2[name]\n",
    "        self.target_critic_1.load_state_dict(target_phi_1)\n",
    "        self.target_critic_2.load_state_dict(target_phi_2)\n",
    "\n",
    "\n",
    "    # train Q1, Q2\n",
    "    def critic_learn(self, states, actions, y_i):\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32).to(self.device)\n",
    "        q_targets = y_i.to(self.device)\n",
    "        \n",
    "        q_1 = self.critic_1([states, actions])\n",
    "        # where q_1 is the predicted value, q_targets is the true val.\n",
    "        loss_1 = F.mse_loss(q_1, q_targets)\n",
    "\n",
    "        # sets the gradients of all parameters of the optimizer to zero. This is necessary to prevent the gradients from accumulating from multiple backpropagation passes.\n",
    "        self.critic_1_optimizer.zero_grad()\n",
    "        #computes the gradients of the loss with respect to all the learnable parameters in the critic network\n",
    "        loss_1.backward()\n",
    "        #updates the parameters of the critic network based on the computed gradients.\n",
    "        self.critic_1_optimizer.step()\n",
    "\n",
    "        q_2 = self.critic_2([states, actions])\n",
    "        loss_2 = F.mse_loss(q_2, q_targets)\n",
    "\n",
    "        self.critic_2_optimizer.zero_grad()\n",
    "        loss_2.backward()\n",
    "        self.critic_2_optimizer.step()\n",
    "\n",
    "    def actor_learn(self, states):\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        mu, std = self.actor(states)\n",
    "        actions, log_pdfs = self.actor.sample_normal(mu, std)\n",
    "        log_pdfs = log_pdfs.squeeze(1)\n",
    "        soft_q_1 = self.critic_1([states, actions])\n",
    "        soft_q_2 = self.critic_2([states, actions])\n",
    "        soft_q = torch.min(soft_q_1, soft_q_2)\n",
    "\n",
    "        loss = torch.mean(self.ALPHA * log_pdfs - soft_q)\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        return float(loss)\n",
    "\n",
    "    def q_target(self, rewards, q_values, dones):\n",
    "        y_k = np.asarray(q_values)\n",
    "        for i in range(q_values.shape[0]):\n",
    "            if dones[i]:\n",
    "                y_k[i] = rewards[i]\n",
    "            else:\n",
    "                y_k[i] = rewards[i] + self.GAMMA * q_values[i]\n",
    "        return torch.tensor(y_k, dtype=torch.float32)\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        self.actor.load_state_dict(torch.load(path + 'Drone_actor_2q.pth'))\n",
    "        self.critic_1.load_state_dict(torch.load(path + 'Drone_critic_12q.pth'))\n",
    "        self.critic_2.load_state_dict(torch.load(path + 'Drone_critic_22q.pth'))\n",
    "\n",
    "\n",
    "    def train(self, max_episode_num, env, behavior_name):\n",
    "        transform = make_depth_transform() # Apply the transformation pipeline\n",
    "\n",
    "        self.param_counter = 0\n",
    "        cnt = 0\n",
    "        # reset target network param.\n",
    "        self.update_target_network(1.0)\n",
    "\n",
    "        for ep in range(int(max_episode_num)):\n",
    "            is_learning = False\n",
    "            frame, episode_reward = 0, 0\n",
    "            # reset the enviornment\n",
    "            env.reset()\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            episode_done = False\n",
    "\n",
    "            # setting up the initial state as an array\n",
    "            #----------------------------------------\n",
    "            np_camera_image = decision_steps.obs[0][0] # Camera Sensor\n",
    "            v_agent_target = decision_steps.obs[1][0] # Agent's position (x,y,z) + Target's position (x,y,z)\n",
    "            image_tensor = np_camera_image     # just a naming for better understanding\n",
    "            transformed_image = transform(image_tensor)\n",
    "            # batch = transformed_image.unsqueeze(0).cuda() # Make a batch of one image\n",
    "            batch = transformed_image.unsqueeze(0) # Make a batch of one image\n",
    "            with torch.inference_mode():\n",
    "                result = model.whole_inference(batch, img_meta=None, rescale=True)\n",
    "            treated_image_2d = result.squeeze().cpu()\n",
    "            treated_image = treated_image_2d.flatten() # converts 2d into 1d\n",
    "            # now concat the two 1d array for state -> SAC\n",
    "            state = np.concatenate((treated_image, v_agent_target), 0)\n",
    "            #-----------------------------------------\n",
    "\n",
    "            while not episode_done:\n",
    "\n",
    "                action = self.get_action(state)\n",
    "                # wrap the action with ActionTuple before sending it to UE.\n",
    "                action = ActionTuple(np.array(action, dtype = np.float32))\n",
    "\n",
    "                env.set_actions(behavior_name, action)\n",
    "                # move the agent along with the action.\n",
    "                env.step()\n",
    "                action = action._continuous # converting ActionTuple to array\n",
    "                next_decision_steps, next_terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "                # if the agent is still on, collect data and add it to buffer.\n",
    "                if next_decision_steps:\n",
    "                    # get the reward.\n",
    "                    train_reward = next_decision_steps.reward[0]\n",
    "\n",
    "                    #----------------------------------------\n",
    "                    np_camera_image = decision_steps.obs[0][0] # Camera Sensor\n",
    "                    v_agent_target = decision_steps.obs[1][0] # Agent's position (x,y,z) + Target's position (x,y,z)\n",
    "                    image_tensor = np_camera_image     # just a naming for better understanding\n",
    "                    transformed_image = transform(image_tensor)\n",
    "                    # batch = transformed_image.unsqueeze(0).cuda() # Make a batch of one image\n",
    "                    batch = transformed_image.unsqueeze(0) # Make a batch of one image\n",
    "                    with torch.inference_mode():\n",
    "                        result = model.whole_inference(batch, img_meta=None, rescale=True)\n",
    "                    treated_image_2d = result.squeeze().cpu()\n",
    "                    treated_image = treated_image_2d.flatten() # converts 2d into 1d\n",
    "                    # now concat the two 1d array for state -> SAC\n",
    "                    next_state = np.concatenate((treated_image, v_agent_target), 0)\n",
    "                    #-----------------------------------------\n",
    "\n",
    "                    episode_reward += next_decision_steps.reward[0]\n",
    "                    # store the data to the buffer\n",
    "                    self.buffer.add_data(state, action, train_reward, next_state, False)\n",
    "                    episode_done = False\n",
    "\n",
    "                # if the agent is off, collect data and add True for done.\n",
    "                if next_terminal_steps:\n",
    "                    # get the reward.\n",
    "                    train_reward = next_terminal_steps.reward[0]\n",
    "\n",
    "                    #----------------------------------------\n",
    "                    np_camera_image = decision_steps.obs[0][0] # Camera Sensor\n",
    "                    v_agent_target = decision_steps.obs[1][0] # Agent's position (x,y,z) + Target's position (x,y,z)\n",
    "                    image_tensor = np_camera_image     # just a naming for better understanding\n",
    "                    transformed_image = transform(image_tensor)\n",
    "                    # batch = transformed_image.unsqueeze(0).cuda() # Make a batch of one image\n",
    "                    batch = transformed_image.unsqueeze(0) # Make a batch of one image\n",
    "                    with torch.inference_mode():\n",
    "                        result = model.whole_inference(batch, img_meta=None, rescale=True)\n",
    "                    treated_image_2d = result.squeeze().cpu()\n",
    "                    treated_image = treated_image_2d.flatten() # converts 2d into 1d\n",
    "                    # now concat the two 1d array for state -> SAC\n",
    "                    next_state = np.concatenate((treated_image, v_agent_target), 0)\n",
    "                    #-----------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "                    episode_reward += next_terminal_steps.reward[0]\n",
    "                    # store the data to the buffer\n",
    "                    self.buffer.add_data(state, action, train_reward, next_state, True)\n",
    "                    episode_done = True\n",
    "\n",
    "                # if buffer has enough data start training.\n",
    "                if self.buffer.buffer_count() > self.BUFFER_SIZE - self.BATCH_SIZE:\n",
    "                    is_learning = True\n",
    "\n",
    "                    states, actions, rewards, next_states, dones = self.buffer.sample_batch(self.BATCH_SIZE)\n",
    "\n",
    "                    # Calculate the Q target value\n",
    "                    with torch.no_grad():\n",
    "                        next_mu, next_std = self.actor(torch.tensor(next_states, dtype=torch.float32).to(self.device))\n",
    "                        next_actions, next_log_pdf = self.actor.sample_normal(next_mu, next_std)\n",
    "\n",
    "                        # convert np to tensor\n",
    "                        tensor_next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "                        tensor_next_actions = torch.tensor(next_actions, dtype=torch.float32)\n",
    "\n",
    "                        # move to CUDA\n",
    "                        tensor_next_states = tensor_next_states.to(self.device)\n",
    "                        tensor_next_actions = tensor_next_actions.to(self.device)\n",
    "\n",
    "                        target_qs_1 = self.target_critic_1([tensor_next_states, tensor_next_actions])\n",
    "                        target_qs_2 = self.target_critic_2([tensor_next_states, tensor_next_actions])\n",
    "                        target_qs = torch.min(target_qs_1, target_qs_2)\n",
    "\n",
    "                        target_qi = target_qs - self.ALPHA * next_log_pdf\n",
    "                        y_i = self.q_target(rewards, target_qi.numpy(), dones)\n",
    "\n",
    "\n",
    "                    self.critic_learn(states, actions, y_i)\n",
    "\n",
    "                    # update Actor and return policy loss\n",
    "                    policy_loss = self.actor_learn(states)\n",
    "\n",
    "                    self.update_target_network(self.TAU)\n",
    "\n",
    "                state = next_state\n",
    "                frame += 1\n",
    "                cnt += 1\n",
    "\n",
    "            # Episode output\n",
    "            print('Episode: ', ep+1, 'Frame: ', frame, 'u Reward: ', episode_reward)\n",
    "\n",
    "            if is_learning:\n",
    "                self.reward_list.append(train_reward)\n",
    "                self.policy_loss.append(policy_loss)\n",
    "\n",
    "            # every 250th run will store another params\n",
    "            if ep % 250 == 0:\n",
    "                torch.save(self.actor.state_dict(), \"./saved_weights/250th/actor_\"+ str(self.param_counter)+\"_2q.pth\")\n",
    "                torch.save(self.critic_1.state_dict(), \"./saved_weights/250th/critic_\"+str(self.param_counter)+\"_12q.pth\")\n",
    "                torch.save(self.critic_2.state_dict(), \"./saved_weights/250th/critic_\"+str(self.param_counter)+\"_22q.pth\")\n",
    "                self.param_counter += 1\n",
    "\n",
    "    def plot_result(self):\n",
    "        fig=plt.figure(figsize=(18, 6))\n",
    "        fig.add_subplot(1, 3, 1)  # 1 row, 3 columns\n",
    "        plt.plot(self.reward_list)\n",
    "\n",
    "        fig.add_subplot(1, 3, 3)\n",
    "        plt.plot(self.policy_loss)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Run Unity Enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_ACTIONS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# env = UnityEnvironment(file_name= \"./Linux_Drone_v0.0.2/Linux_Drone_with.x86_64\", base_port=5004)\n",
    "env = UnityEnvironment(file_name= None, base_port=5004)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "behavior_names = list(env.behavior_specs.keys())\n",
    "behavior_name = behavior_names[0]\n",
    "decision_steps, terminal_steps = env.get_steps(behavior_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "7071"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GET N_STATES or len_STATES rather\n",
    "transform = make_depth_transform() # Apply the transformation pipeline\n",
    "#----------------------------------------\n",
    "np_camera_image = decision_steps.obs[0][0] # Camera Sensor\n",
    "v_agent_target = decision_steps.obs[1][0] # Agent's position (x,y,z) + Target's position (x,y,z)\n",
    "image_tensor = np_camera_image     # just a naming for better understanding\n",
    "transformed_image = transform(image_tensor)\n",
    "# batch = transformed_image.unsqueeze(0).cuda() # Make a batch of one image\n",
    "batch = transformed_image.unsqueeze(0) # Make a batch of one image\n",
    "with torch.inference_mode():\n",
    "    result = model.whole_inference(batch, img_meta=None, rescale=True)\n",
    "treated_image_2d = result.squeeze().cpu()\n",
    "treated_image = treated_image_2d.flatten() # converts 2d into 1d\n",
    "# now concat the two 1d array for state -> SAC\n",
    "state = np.concatenate((treated_image, v_agent_target), 0)\n",
    "#-----------------------------------------\n",
    "N_STATES = len(state)\n",
    "N_STATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now [ 17.922789   17.922789   17.922789  ... -30.26        3.4052465\n",
      "  -0.67     ]\n",
      "Episode:  1 Frame:  106 u Reward:  -9.474193572998047\n",
      "now [ 27.70325  27.70325  27.70325 ... -30.26      3.37313  -0.67   ]\n",
      "Episode:  2 Frame:  85 u Reward:  -10.6334228515625\n",
      "now [ 17.865644  17.865644  17.865644 ... -30.26       3.383378  -0.67    ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[81], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m agent \u001B[38;5;241m=\u001B[39m SACagent(N_STATES, N_ACTIONS)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# usually 30K is enough.\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m30000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbehavior_name\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[80], line 177\u001B[0m, in \u001B[0;36mSACagent.train\u001B[0;34m(self, max_episode_num, env, behavior_name)\u001B[0m\n\u001B[1;32m    175\u001B[0m batch \u001B[38;5;241m=\u001B[39m transformed_image\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;66;03m# Make a batch of one image\u001B[39;00m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39minference_mode():\n\u001B[0;32m--> 177\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwhole_inference\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimg_meta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrescale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    178\u001B[0m treated_image_2d \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39mcpu()\n\u001B[1;32m    179\u001B[0m treated_image \u001B[38;5;241m=\u001B[39m treated_image_2d\u001B[38;5;241m.\u001B[39mflatten() \u001B[38;5;66;03m# converts 2d into 1d\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-ateneo.edu/Thesis/Lee_RL/dinov2/eval/depth/models/depther/encoder_decoder.py:135\u001B[0m, in \u001B[0;36mDepthEncoderDecoder.whole_inference\u001B[0;34m(self, img, img_meta, rescale, size)\u001B[0m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwhole_inference\u001B[39m(\u001B[38;5;28mself\u001B[39m, img, img_meta, rescale, size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    134\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Inference with full image.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 135\u001B[0m     depth_pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimg_meta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrescale\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m depth_pred\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-ateneo.edu/Thesis/Lee_RL/dinov2/eval/depth/models/depther/encoder_decoder.py:72\u001B[0m, in \u001B[0;36mDepthEncoderDecoder.encode_decode\u001B[0;34m(self, img, img_metas, rescale, size)\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mencode_decode\u001B[39m(\u001B[38;5;28mself\u001B[39m, img, img_metas, rescale\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     70\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Encode images with backbone and decode into a depth estimation\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;124;03m    map of the same size as input.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 72\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_feat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decode_head_forward_test(x, img_metas)\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;66;03m# crop the pred depth to the certain range.\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-ateneo.edu/Thesis/Lee_RL/dinov2/eval/depth/models/depther/encoder_decoder.py:64\u001B[0m, in \u001B[0;36mDepthEncoderDecoder.extract_feat\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mextract_feat\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m     63\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Extract features from images.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 64\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackbone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwith_neck:\n\u001B[1;32m     66\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mneck(x)\n",
      "File \u001B[0;32m~/miniconda3/envs/rl2/lib/python3.9/site-packages/torch/nn/modules/module.py:1538\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1535\u001B[0m     bw_hook \u001B[38;5;241m=\u001B[39m hooks\u001B[38;5;241m.\u001B[39mBackwardHook(\u001B[38;5;28mself\u001B[39m, full_backward_hooks, backward_pre_hooks)\n\u001B[1;32m   1536\u001B[0m     args \u001B[38;5;241m=\u001B[39m bw_hook\u001B[38;5;241m.\u001B[39msetup_input_hook(args)\n\u001B[0;32m-> 1538\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1539\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks:\n\u001B[1;32m   1540\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m hook_id, hook \u001B[38;5;129;01min\u001B[39;00m (\n\u001B[1;32m   1541\u001B[0m         \u001B[38;5;241m*\u001B[39m_global_forward_hooks\u001B[38;5;241m.\u001B[39mitems(),\n\u001B[1;32m   1542\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks\u001B[38;5;241m.\u001B[39mitems(),\n\u001B[1;32m   1543\u001B[0m     ):\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-ateneo.edu/Thesis/Lee_RL/dinov2/models/vision_transformer.py:274\u001B[0m, in \u001B[0;36mDinoVisionTransformer.get_intermediate_layers\u001B[0;34m(self, x, n, reshape, return_class_token, norm)\u001B[0m\n\u001B[1;32m    272\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_intermediate_layers_chunked(x, n)\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 274\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_intermediate_layers_not_chunked\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m norm:\n\u001B[1;32m    276\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(out) \u001B[38;5;28;01mfor\u001B[39;00m out \u001B[38;5;129;01min\u001B[39;00m outputs]\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-ateneo.edu/Thesis/Lee_RL/dinov2/models/vision_transformer.py:243\u001B[0m, in \u001B[0;36mDinoVisionTransformer._get_intermediate_layers_not_chunked\u001B[0;34m(self, x, n)\u001B[0m\n\u001B[1;32m    241\u001B[0m blocks_to_take \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mrange\u001B[39m(total_block_len \u001B[38;5;241m-\u001B[39m n, total_block_len) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(n, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m n\n\u001B[1;32m    242\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, blk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks):\n\u001B[0;32m--> 243\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mblk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    244\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m blocks_to_take:\n\u001B[1;32m    245\u001B[0m         output\u001B[38;5;241m.\u001B[39mappend(x)\n",
      "File \u001B[0;32m~/miniconda3/envs/rl2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-ateneo.edu/Thesis/Lee_RL/dinov2/layers/block.py:254\u001B[0m, in \u001B[0;36mNestedTensorBlock.forward\u001B[0;34m(self, x_or_x_list)\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x_or_x_list):\n\u001B[1;32m    253\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x_or_x_list, Tensor):\n\u001B[0;32m--> 254\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_or_x_list\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    255\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x_or_x_list, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    256\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m XFORMERS_AVAILABLE:\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-ateneo.edu/Thesis/Lee_RL/dinov2/layers/block.py:113\u001B[0m, in \u001B[0;36mBlock.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    112\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m attn_residual_func(x)\n\u001B[0;32m--> 113\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[43mffn_residual_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-ateneo.edu/Thesis/Lee_RL/dinov2/layers/block.py:94\u001B[0m, in \u001B[0;36mBlock.forward.<locals>.ffn_residual_func\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mffn_residual_func\u001B[39m(x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m---> 94\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mls2(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/miniconda3/envs/rl2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-ateneo.edu/Thesis/Lee_RL/dinov2/layers/mlp.py:38\u001B[0m, in \u001B[0;36mMlp.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     36\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mact(x)\n\u001B[1;32m     37\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdrop(x)\n\u001B[0;32m---> 38\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdrop(x)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/miniconda3/envs/rl2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/rl2/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "agent = SACagent(N_STATES, N_ACTIONS)\n",
    "# usually 30K is enough.\n",
    "agent.train(30000, env, behavior_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent.plot_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
